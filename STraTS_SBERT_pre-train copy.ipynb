{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "#from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import nn\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.layers import Embedding, Layer, Input, Dense, Lambda, Concatenate, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "# this file is needed for dropout operations, as the code used for STraTS is based on a very old Keras version\n",
    "import smart_cond_mod as sc\n",
    "\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "tf.keras.utils.set_random_seed(1)\n",
    "random.seed(100)\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/mimic_and_sbert_for_thesis.pkl\", \"rb\") as pfile:\n",
    "    raw_data = pickle.load(pfile)\n",
    "mimic = raw_data[0]\n",
    "meta = raw_data[1]\n",
    "train_ind = raw_data[2]\n",
    "valid_ind = raw_data[3]\n",
    "test_ind = raw_data[4]\n",
    "data = mimic\n",
    "oc = meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = oc['SUBJECT_ID'].tolist()\n",
    "labels = oc['in_hospital_sepsis'].tolist()\n",
    "\n",
    "new_patient_ids = []\n",
    "new_labels = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "  # print(i)\n",
    "  if ids[i] in new_patient_ids:\n",
    "    continue\n",
    "  else:\n",
    "    new_patient_ids.append(ids[i])\n",
    "    new_labels.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 33592, 1: 3263})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# data ratio\n",
    "Counter(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23587\n",
      "31708\n",
      "7371\n",
      "9894\n",
      "5897\n",
      "7803\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x, x_test, y, y_test = train_test_split(new_patient_ids, new_labels, test_size=0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "# train\n",
    "train_ind = []\n",
    "\n",
    "ts_ind = oc['ts_ind'].tolist()\n",
    "# ids = ids\n",
    "\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_train:\n",
    "    train_ind.append(ts_ind[i])\n",
    "\n",
    "# number of train patients\n",
    "print(len(x_train))\n",
    "# number of train instances\n",
    "print(len(train_ind))\n",
    "# to np.array\n",
    "train_ind = np.array(train_ind)\n",
    "\n",
    "test_ind = []\n",
    "\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_test:\n",
    "    test_ind.append(ts_ind[i])\n",
    "\n",
    "# number of test patients\n",
    "print(len(x_test))\n",
    "# number of test instances\n",
    "print(len(test_ind))\n",
    "# to np.array\n",
    "test_ind = np.array(test_ind)\n",
    "\n",
    "valid_ind = []\n",
    "\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_val:\n",
    "    valid_ind.append(ts_ind[i])\n",
    "\n",
    "# number of test patients\n",
    "print(len(x_val))\n",
    "# number of test instances\n",
    "print(len(valid_ind))\n",
    "# to np.array\n",
    "valid_ind = np.array(valid_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_window = 1 # hours that the output vector represents. 1 because i want to learn to predict 1 hour many times\n",
    "obs_windows = range(20, 124, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79022it [00:00, 389272.63it/s]\n",
      "100%|██████████| 26/26 [03:16<00:00,  7.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# Remove test patients.\n",
    "data = data.merge(oc[['ts_ind', 'SUBJECT_ID']], on='ts_ind', how='left')\n",
    "test_sub = oc.loc[oc.ts_ind.isin(test_ind)].SUBJECT_ID.unique()\n",
    "data = data.loc[~data.SUBJECT_ID.isin(test_sub)]\n",
    "oc = oc.loc[~oc.SUBJECT_ID.isin(test_sub)]\n",
    "data.drop(columns=['SUBJECT_ID'], inplace=True)\n",
    "# Fix age.\n",
    "data.loc[(data.variable=='Age')&(data.value>200), 'value'] = 91.4\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "N = data.ts_ind.max()+1\n",
    "demo = np.zeros((N, D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds==0)*1 + (stds!=0)*stds\n",
    "demo = (demo-means)/stds\n",
    "# Get variable indices.\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Find max_len.\n",
    "fore_max_len = 880\n",
    "# Get forecast inputs and outputs.\n",
    "fore_times_ip = []\n",
    "fore_values_ip = []\n",
    "fore_varis_ip = []\n",
    "fore_op = []\n",
    "fore_inds = []\n",
    "def f(x):\n",
    "    mask = [0 for i in range(V)]\n",
    "    values = [0 for i in range(V)]\n",
    "    for vv in x:\n",
    "        v = int(vv[0])-1\n",
    "        mask[v] = 1\n",
    "        values[v] = vv[1]\n",
    "    return values+mask\n",
    "def pad(x):\n",
    "    return x+[0]*(fore_max_len-len(x))\n",
    "for w in tqdm(obs_windows):\n",
    "    pred_data = data.loc[(data.hour>=w)&(data.hour<=w+pred_window)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value':'first'}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value':list}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data['vind_value'].apply(f)\n",
    "    obs_data = data.loc[(data.hour<w)&(data.hour>=w-24)]\n",
    "    obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "    obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "    obs_data = obs_data.groupby('ts_ind').agg({'vind':list, 'hour':list, 'value':list}).reset_index()\n",
    "    obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "    for col in ['vind', 'hour', 'value']:\n",
    "        obs_data[col] = obs_data[col].apply(pad)\n",
    "    fore_op.append(np.array(list(obs_data.vind_value)))\n",
    "    fore_inds.append(np.array(list(obs_data.ts_ind)))\n",
    "    fore_times_ip.append(np.array(list(obs_data.hour)))\n",
    "    fore_values_ip.append(np.array(list(obs_data.value)))\n",
    "    fore_varis_ip.append(np.array(list(obs_data.vind)))\n",
    "del data\n",
    "fore_times_ip = np.concatenate(fore_times_ip, axis=0)\n",
    "fore_values_ip = np.concatenate(fore_values_ip, axis=0)\n",
    "fore_varis_ip = np.concatenate(fore_varis_ip, axis=0)\n",
    "fore_op = np.concatenate(fore_op, axis=0)\n",
    "fore_inds = np.concatenate(fore_inds, axis=0)\n",
    "fore_demo = demo[fore_inds]\n",
    "# Get train and valid ts_ind for forecast task.\n",
    "train_sub = oc.loc[oc.ts_ind.isin(train_ind)].SUBJECT_ID.unique()\n",
    "valid_sub = oc.loc[oc.ts_ind.isin(valid_ind)].SUBJECT_ID.unique()\n",
    "rem_sub = oc.loc[~oc.SUBJECT_ID.isin(np.concatenate((train_ind, valid_ind)))].SUBJECT_ID.unique()\n",
    "bp = int(0.8*len(rem_sub))\n",
    "train_sub = np.concatenate((train_sub, rem_sub[:bp]))\n",
    "valid_sub = np.concatenate((valid_sub, rem_sub[bp:]))\n",
    "train_ind = oc.loc[oc.SUBJECT_ID.isin(train_sub)].ts_ind.unique() # Add remaining ts_ind s of train subjects.\n",
    "valid_ind = oc.loc[oc.SUBJECT_ID.isin(valid_sub)].ts_ind.unique() # Add remaining ts_ind s of train subjects.\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ind = np.argwhere(np.in1d(fore_inds, train_ind)).flatten()\n",
    "valid_ind = np.argwhere(np.in1d(fore_inds, valid_ind)).flatten()\n",
    "\n",
    "# INPUT\n",
    "# 4*sequencenumber*880\n",
    "# (demographics, hours, values, feature names) * (observation sequences) * 880\n",
    "fore_train_ip = [ip[train_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "fore_valid_ip = [ip[valid_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "del fore_times_ip, fore_values_ip, fore_varis_ip, demo, fore_demo\n",
    "\n",
    "# OUTPUT\n",
    "# sequencenumber * (2 * # of features)\n",
    "# sequencenumber * (feature values + feature mask)\n",
    "# for SBERT and TF-IDF, noise: sequencenumber * (183*2)\n",
    "# for BASE and TF-IDF: sequencenumber * (133*2)\n",
    "fore_train_op = fore_op[train_ind]\n",
    "fore_valid_op = fore_op[valid_ind]\n",
    "del fore_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395850"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of 24 hour sequences\n",
    "len(fore_train_ip[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8,  10,  13,  21,  24,  24,  24,  26,  29,  29,  29,  29,  29,\n",
       "        29,  29,  29,  29,  29,  29,  29,  29,  29,  29,  29,  29,  29,\n",
       "        30,  43,  43,  43,  43,  43,  43,  43,  43,  43,  43,  43,  43,\n",
       "        43,  43,  43,  43,  43,  43,  43,  43,  43,  44,  44,  44,  44,\n",
       "        44,  44,  44,  44,  44,  44,  44,  44,  44,  44,  44,  44,  44,\n",
       "        44,  44,  44,  44,  45,  45,  45,  45,  45,  45,  45,  45,  45,\n",
       "        45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  49,\n",
       "        49,  49,  49,  50,  50,  50,  50,  52,  52,  52,  52,  52,  52,\n",
       "        52,  52,  52,  52,  52,  52,  52,  52,  52,  52,  52,  52,  52,\n",
       "        52,  54,  57,  58,  60,  61,  76,  76,  76,  76,  76,  76,  76,\n",
       "        76,  76,  76,  76,  76,  76,  76,  76,  76,  76,  76,  77,  78,\n",
       "        79,  80,  88,  94,  94,  94,  94,  94,  94,  94,  94,  95,  95,\n",
       "        95,  95,  95,  95,  95,  95,  95,  95,  95,  95,  95,  95,  95,\n",
       "        95,  95,  95,  95, 100, 101, 104, 105, 105, 107, 108, 108, 108,\n",
       "       109, 110, 112, 113, 114, 114, 114, 114, 114, 114, 114, 114, 114,\n",
       "       114, 114, 114, 114, 114, 114, 114, 114, 114, 116, 116, 116, 116,\n",
       "       116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116,\n",
       "       116, 117, 118, 118, 118, 123, 123, 123, 123, 123, 123, 127, 127,\n",
       "       127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 130, 131, 131,\n",
       "       131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 133, 133,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fore_train_ip[3][19999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One feature ID Full: Counter({95: 25, 114: 25, 9: 24, 29: 24, 76: 24, 116: 24, 52: 23, 0: 20, 28: 16, 37: 16, 39: 16, 71: 16, 93: 16, 94: 16, 111: 16, 126: 16, 127: 16, 123: 12, 38: 9, 134: 9, 135: 9, 136: 9, 137: 9, 138: 9, 139: 9, 140: 9, 141: 9, 142: 9, 143: 9, 144: 9, 145: 9, 146: 9, 147: 9, 148: 9, 149: 9, 150: 9, 151: 9, 152: 9, 153: 9, 154: 9, 155: 9, 156: 9, 157: 9, 158: 9, 159: 9, 160: 9, 161: 9, 162: 9, 163: 9, 164: 9, 165: 9, 166: 9, 167: 9, 168: 9, 169: 9, 170: 9, 171: 9, 172: 9, 173: 9, 174: 9, 175: 9, 176: 9, 177: 9, 178: 9, 179: 9, 180: 9, 181: 9, 182: 9, 183: 9, 43: 7, 44: 7, 45: 7, 11: 4, 49: 4, 97: 4, 99: 4, 124: 4, 132: 4, 48: 3, 65: 3, 8: 1, 10: 1, 13: 1, 19: 1, 21: 1, 24: 1, 26: 1, 50: 1, 54: 1, 57: 1, 60: 1, 70: 1, 77: 1, 78: 1, 79: 1, 80: 1, 100: 1, 101: 1, 104: 1, 107: 1, 108: 1, 112: 1, 113: 1, 118: 1, 130: 1})\n",
      "One feature ID Full: [  8   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9\n",
      "   9   9   9   9   9   9   9  10  11  11  11  11  13  19  21  24  26  28\n",
      "  28  28  28  28  28  28  28  28  28  28  28  28  28  28  28  29  29  29\n",
      "  29  29  29  29  29  29  29  29  29  29  29  29  29  29  29  29  29  29\n",
      "  29  29  29  37  37  37  37  37  37  37  37  37  37  37  37  37  37  37\n",
      "  37  38  38  38  38  38  38  38  38  38  39  39  39  39  39  39  39  39\n",
      "  39  39  39  39  39  39  39  39  43  43  43  43  43  43  43  44  44  44\n",
      "  44  44  44  44  45  45  45  45  45  45  45  48  48  48  49  49  49  49\n",
      "  50  52  52  52  52  52  52  52  52  52  52  52  52  52  52  52  52  52\n",
      "  52  52  52  52  52  52  54  57  60  65  65  65  70  71  71  71  71  71\n",
      "  71  71  71  71  71  71  71  71  71  71  71  76  76  76  76  76  76  76\n",
      "  76  76  76  76  76  76  76  76  76  76  76  76  76  76  76  76  76  77\n",
      "  78  79  80  93  93  93  93  93  93  93  93  93  93  93  93  93  93  93\n",
      "  93  94  94  94  94  94  94  94  94  94  94  94  94  94  94  94  94  95\n",
      "  95  95  95  95  95  95  95  95  95  95  95  95  95  95  95  95  95  95\n",
      "  95  95  95  95  95  95  97  97  97  97  99  99  99  99 100 101 104 107\n",
      " 108 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 112\n",
      " 113 114 114 114 114 114 114 114 114 114 114 114 114 114 114 114 114 114\n",
      " 114 114 114 114 114 114 114 114 116 116 116 116 116 116 116 116 116 116\n",
      " 116 116 116 116 116 116 116 116 116 116 116 116 116 116 118 123 123 123\n",
      " 123 123 123 123 123 123 123 123 123 124 124 124 124 126 126 126 126 126\n",
      " 126 126 126 126 126 126 126 126 126 126 126 127 127 127 127 127 127 127\n",
      " 127 127 127 127 127 127 127 127 127 130 132 132 132 132 134 134 134 134\n",
      " 134 134 134 134 134 135 135 135 135 135 135 135 135 135 136 136 136 136\n",
      " 136 136 136 136 136 137 137 137 137 137 137 137 137 137 138 138 138 138\n",
      " 138 138 138 138 138 139 139 139 139 139 139 139 139 139 140 140 140 140\n",
      " 140 140 140 140 140 141 141 141 141 141 141 141 141 141 142 142 142 142\n",
      " 142 142 142 142 142 143 143 143 143 143 143 143 143 143 144 144 144 144\n",
      " 144 144 144 144 144 145 145 145 145 145 145 145 145 145 146 146 146 146\n",
      " 146 146 146 146 146 147 147 147 147 147 147 147 147 147 148 148 148 148\n",
      " 148 148 148 148 148 149 149 149 149 149 149 149 149 149 150 150 150 150\n",
      " 150 150 150 150 150 151 151 151 151 151 151 151 151 151 152 152 152 152\n",
      " 152 152 152 152 152 153 153 153 153 153 153 153 153 153 154 154 154 154\n",
      " 154 154 154 154 154 155 155 155 155 155 155 155 155 155 156 156 156 156\n",
      " 156 156 156 156 156 157 157 157 157 157 157 157 157 157 158 158 158 158\n",
      " 158 158 158 158 158 159 159 159 159 159 159 159 159 159 160 160 160 160\n",
      " 160 160 160 160 160 161 161 161 161 161 161 161 161 161 162 162 162 162\n",
      " 162 162 162 162 162 163 163 163 163 163 163 163 163 163 164 164 164 164\n",
      " 164 164 164 164 164 165 165 165 165 165 165 165 165 165 166 166 166 166\n",
      " 166 166 166 166 166 167 167 167 167 167 167 167 167 167 168 168 168 168\n",
      " 168 168 168 168 168 169 169 169 169 169 169 169 169 169 170 170 170 170\n",
      " 170 170 170 170 170 171 171 171 171 171 171 171 171 171 172 172 172 172\n",
      " 172 172 172 172 172 173 173 173 173 173 173 173 173 173 174 174 174 174\n",
      " 174 174 174 174 174 175 175 175 175 175 175 175 175 175 176 176 176 176\n",
      " 176 176 176 176 176 177 177 177 177 177 177 177 177 177 178 178 178 178\n",
      " 178 178 178 178 178 179 179 179 179 179 179 179 179 179 180 180 180 180\n",
      " 180 180 180 180 180 181 181 181 181 181 181 181 181 181 182 182 182 182\n",
      " 182 182 182 182 182 183 183 183 183 183 183 183 183 183   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "#print(\"Output length Full:\", len(fore_train_op), \"Output length Single\", len(fore_train_op[0]))\n",
    "#print(\"One Output Full:\\n\", fore_train_op[1])\n",
    "\n",
    "#print(\"features length Full:\", len(fore_train_ip[3]), \"features length Single\", len(fore_train_ip[3][0]))\n",
    "\n",
    "print(\"One feature ID Full:\", Counter(fore_train_ip[3][2]))\n",
    "print(\"One feature ID Full:\", fore_train_ip[3][2])\n",
    "\n",
    "#print(\"One feature value Full:\", fore_train_ip[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All_Text_Features</th>\n",
       "      <th>Min1_Text_Features</th>\n",
       "      <th>SomeMiss_Text_Features</th>\n",
       "      <th>all_text_miss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74.199571</td>\n",
       "      <td>87.928508</td>\n",
       "      <td>25.800429</td>\n",
       "      <td>12.071492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   All_Text_Features  Min1_Text_Features  SomeMiss_Text_Features  \\\n",
       "0          74.199571           87.928508               25.800429   \n",
       "\n",
       "   all_text_miss  \n",
       "0      12.071492  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ids = set(range(134,184,1))\n",
    "missing_text_list = []\n",
    "all_seq = len(fore_train_ip[3])\n",
    "# for each seq look at feature names\n",
    "for i in range(len(fore_train_ip[3])):\n",
    "    # count which feature names are in sequence\n",
    "    counts = Counter(fore_train_ip[3][i])\n",
    "\n",
    "    # append length of list containing all missing text features in sequence\n",
    "    missing_text_list.append(len(list(text_ids - counts.keys())))\n",
    "\n",
    "# for all 24 hour sequences:\n",
    "# all text features present: no text features missing = Counter(missing_text_list)[0]\n",
    "full_text_features = Counter(missing_text_list)[0]\n",
    "# all sequences with some text missing: all seq - sequences with full text features\n",
    "missing_some_text = all_seq-full_text_features\n",
    "# full text missing: all 50 features missing\n",
    "missing_50 = Counter(missing_text_list)[50]\n",
    "# at least 1 text feature: all sequences - sequences where all 50 features missing\n",
    "min_1_text =  all_seq - missing_50\n",
    "d = {\"All_Text_Features\": [full_text_features / all_seq*100], \"Min1_Text_Features\": [min_1_text / all_seq*100], \"SomeMiss_Text_Features\": [missing_some_text / all_seq*100], \"all_text_miss\": [missing_50 / all_seq*100]}\n",
    "text_percentages = pd.DataFrame.from_dict(d)\n",
    "text_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all text\n",
    "full_text_features = Counter(missing_text_list)[0]\n",
    "# all sequences with some text missing\n",
    "missing_some_text = len(fore_train_ip[3])-Counter(missing_text_list)[0]\n",
    "# full text missing\n",
    "missing_50 = Counter(missing_text_list)[50]\n",
    "# of sequences with out all text sequences where at least 1 text feature is in\n",
    "min_1_text =  full_text_features - missing_50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5326"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_some_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3241"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13882"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_1_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17123"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22449"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_seq = len(fore_train_ip[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50 / 200 *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    }
   ],
   "source": [
    "counts = Counter(fore_train_ip[3][0])\n",
    "if counts.keys().__contains__(100000):\n",
    "    print(\"true\")\n",
    "else:\n",
    "    print(\"false\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res(y_true, y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    minrp = np.minimum(precision, recall).max()\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    return [roc_auc, pr_auc, minrp]\n",
    "\n",
    "\n",
    "#class_weights = compute_class_weight(class_weight='balanced', classes=[0,1], y=train_op)\n",
    "#def mortality_loss(y_true, y_pred):\n",
    "    #sample_weights = (1-y_true)*class_weights[0] + y_true*class_weights[1]\n",
    "    #bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    #return K.mean(sample_weights*bce, axis=-1)\n",
    "\n",
    "\n",
    "# var_weights = np.sum(fore_train_op[:, V:], axis=0)\n",
    "# var_weights[var_weights==0] = var_weights.max()\n",
    "# var_weights = var_weights.max()/var_weights\n",
    "# var_weights = var_weights.reshape((1, V))\n",
    "def forecast_loss(y_true, y_pred):\n",
    "    return K.sum(y_true[:,V:]*(y_true[:,:V]-y_pred)**2, axis=-1)\n",
    "                                          \n",
    "def get_min_loss(weight):\n",
    "    def min_loss(y_true, y_pred):\n",
    "        return weight*y_pred\n",
    "    return min_loss\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, validation_data, batch_size):\n",
    "        self.val_x, self.val_y = validation_data\n",
    "        self.batch_size = batch_size\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.val_x, verbose=0, batch_size=self.batch_size)\n",
    "        if type(y_pred)==type([]):\n",
    "            y_pred = y_pred[0]\n",
    "        precision, recall, thresholds = precision_recall_curve(self.val_y, y_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        roc_auc = roc_auc_score(self.val_y, y_pred)\n",
    "        logs['custom_metric'] = pr_auc + roc_auc\n",
    "        print ('val_aucs:', pr_auc, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVE(Layer):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        super(CVE, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W1 = self.add_weight(name='CVE_W1',\n",
    "                            shape=(1, self.hid_units),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        self.b1 = self.add_weight(name='CVE_b1',\n",
    "                            shape=(self.hid_units,),\n",
    "                            initializer='zeros',\n",
    "                            trainable=True)\n",
    "        self.W2 = self.add_weight(name='CVE_W2',\n",
    "                            shape=(self.hid_units, self.output_dim),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        super(CVE, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = K.expand_dims(x, axis=-1)\n",
    "        x = K.dot(K.tanh(K.bias_add(K.dot(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape + (self.output_dim,)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, hid_dim):\n",
    "        self.hid_dim = hid_dim\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        self.W = self.add_weight(shape=(d, self.hid_dim), name='Att_W',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.hid_dim,), name='Att_b',\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(shape=(self.hid_dim,1), name='Att_u',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = K.dot(K.tanh(K.bias_add(K.dot(x,self.W), self.b)), self.u)\n",
    "        mask = K.expand_dims(mask, axis=-1)\n",
    "        attn_weights = mask*attn_weights + (1-mask)*mask_value\n",
    "        attn_weights = K.softmax(attn_weights, axis=-2)\n",
    "        return attn_weights\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (1,)\n",
    "\n",
    "\n",
    "class Transformer(Layer):\n",
    "\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0):\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = K.epsilon() * K.epsilon()\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        if self.dk==None:\n",
    "            self.dk = d//self.h\n",
    "        if self.dv==None:\n",
    "            self.dv = d//self.h\n",
    "        if self.dff==None:\n",
    "            self.dff = 2*d\n",
    "        self.Wq = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wq',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wk = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wk',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wv = self.add_weight(shape=(self.N, self.h, d, self.dv), name='Wv',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wo = self.add_weight(shape=(self.N, self.dv*self.h, d), name='Wo',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.W1 = self.add_weight(shape=(self.N, d, self.dff), name='W1',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b1 = self.add_weight(shape=(self.N, self.dff), name='b1',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.W2 = self.add_weight(shape=(self.N, self.dff, d), name='W2',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b2 = self.add_weight(shape=(self.N, d), name='b2',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.gamma = self.add_weight(shape=(2*self.N,), name='gamma',\n",
    "                                 initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(shape=(2*self.N,), name='beta',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Transformer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask, mask_value=-1e-30):\n",
    "        mask = K.expand_dims(mask, axis=-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = K.dot(x, self.Wq[i,j,:,:])\n",
    "                k = K.permute_dimensions(K.dot(x, self.Wk[i,j,:,:]), (0,2,1))\n",
    "                v = K.dot(x, self.Wv[i,j,:,:])\n",
    "                A = K.batch_dot(q,k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask*A + (1-mask)*mask_value\n",
    "                # Mask for attention dropout.\n",
    "                def dropped_A():\n",
    "                    dp_mask = K.cast((K.random_uniform(shape=array_ops.shape(A))>=self.dropout), K.floatx())\n",
    "                    return A*dp_mask + (1-dp_mask)*mask_value\n",
    "                A = sc.smart_cond(K.learning_phase(), dropped_A, lambda: array_ops.identity(A))\n",
    "                A = K.softmax(A, axis=-1)\n",
    "                mha_ops.append(K.batch_dot(A,v))\n",
    "            conc = K.concatenate(mha_ops, axis=-1)\n",
    "            proj = K.dot(conc, self.Wo[i,:,:])\n",
    "            # Dropout.\n",
    "            proj = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(proj, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(proj))\n",
    "            # Add & LN\n",
    "            x = x+proj\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i] + self.beta[2*i]\n",
    "            # FFN\n",
    "            ffn_op = K.bias_add(K.dot(K.relu(K.bias_add(K.dot(x, self.W1[i,:,:]), self.b1[i,:])),\n",
    "                           self.W2[i,:,:]), self.b2[i,:,])\n",
    "            # Dropout.\n",
    "            ffn_op = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(ffn_op, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(ffn_op))\n",
    "            # Add & LN\n",
    "            x = x+ffn_op\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i+1] + self.beta[2*i+1]\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "def build_strats(D, max_len, V, d, N, he, dropout, forecast=False):\n",
    "    demo = Input(shape=(D,))\n",
    "    demo_enc = Dense(2*d, activation='tanh')(demo)\n",
    "    demo_enc = Dense(d, activation='tanh')(demo_enc)\n",
    "    varis = Input(shape=(max_len,))\n",
    "    values = Input(shape=(max_len,))\n",
    "    times = Input(shape=(max_len,))\n",
    "    varis_emb = Embedding(V+1, d)(varis)\n",
    "    cve_units = int(np.sqrt(d))\n",
    "    values_emb = CVE(cve_units, d)(values)\n",
    "    times_emb = CVE(cve_units, d)(times)\n",
    "    comb_emb = Add()([varis_emb, values_emb, times_emb]) # b, L, d\n",
    "#     demo_enc = Lambda(lambda x:K.expand_dims(x, axis=-2))(demo_enc) # b, 1, d\n",
    "#     comb_emb = Concatenate(axis=-2)([demo_enc, comb_emb]) # b, L+1, d\n",
    "    mask = Lambda(lambda x:K.clip(x,0,1))(varis) # b, L\n",
    "#     mask = Lambda(lambda x:K.concatenate((K.ones_like(x)[:,0:1], x), axis=-1))(mask) # b, L+1\n",
    "    cont_emb = Transformer(N, he, dk=None, dv=None, dff=None, dropout=dropout)(comb_emb, mask=mask)\n",
    "    attn_weights = Attention(2*d)(cont_emb, mask=mask)\n",
    "    fused_emb = Lambda(lambda x:K.sum(x[0]*x[1], axis=-2))([cont_emb, attn_weights])\n",
    "    conc = Concatenate(axis=-1)([fused_emb, demo_enc])\n",
    "    fore_op = Dense(V)(conc)\n",
    "    op = Dense(1, activation='sigmoid')(fore_op)\n",
    "    model = Model([demo, times, values, varis], op)\n",
    "    if forecast:\n",
    "        fore_model = Model([demo, times, values, varis], fore_op)\n",
    "        return [model, fore_model]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-23 15:16:57.274924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31133 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b2:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 880)]                0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 880)]                0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 880)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 880, 50)              9200      ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " cve (CVE)                   (None, 880, 50)              364       ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " cve_1 (CVE)                 (None, 880, 50)              364       ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 880, 50)              0         ['embedding[0][0]',           \n",
      "                                                                     'cve[0][0]',                 \n",
      "                                                                     'cve_1[0][0]']               \n",
      "                                                                                                  \n",
      " lambda (Lambda)             (None, 880)                  0         ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " transformer (Transformer)   (None, 880, 50)              39508     ['add[0][0]',                 \n",
      "                                                                     'lambda[0][0]']              \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)        [(None, 2)]                  0         []                            \n",
      "                                                                                                  \n",
      " attention (Attention)       (None, 880, 1)               5200      ['transformer[0][0]',         \n",
      "                                                                     'lambda[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 100)                  300       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)           (None, 50)                   0         ['transformer[0][0]',         \n",
      "                                                                     'attention[0][0]']           \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 50)                   5050      ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 100)                  0         ['lambda_1[0][0]',            \n",
      "                                                                     'dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 183)                  18483     ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 78469 (306.52 KB)\n",
      "Trainable params: 78469 (306.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#surely can be done more elegant...\n",
    "fore_savepath = './models/EXP3_STraTS_SBERT'\n",
    "\n",
    "train_FILE_PATH = Path(f'{fore_savepath}/train_losses.csv')\n",
    "val_FILE_PATH = Path(f'{fore_savepath}/val_losses.csv')\n",
    "\n",
    "# initialize model parameters\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, len(fore_train_op), 5\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model, fore_model =  build_strats(D, fore_max_len, V, d, N, he, dropout, forecast=True)\n",
    "print(fore_model.summary())\n",
    "lossfunction = forecast_loss\n",
    "opt = tf.keras.optimizers.Adam(lr)\n",
    "fore_model.compile(loss=lossfunction, optimizer=opt)\n",
    "\n",
    "# initialize checkpoint manager\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=fore_model)\n",
    "manager = tf.train.CheckpointManager(ckpt, f'{fore_savepath}', max_to_keep=3)\n",
    "\n",
    "# define training procedure\n",
    "def train_and_checkpoint(net, manager):\n",
    "  # initialize loss, etc\n",
    "  best_val_loss = np.inf\n",
    "  N_fore = len(fore_train_op)\n",
    "  train_losses = []\n",
    "  val_losses = []\n",
    "\n",
    "  # load or create model\n",
    "  ckpt.restore(manager.latest_checkpoint)\n",
    "  if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "  else:\n",
    "    print(\"Initializing from scratch.\")\n",
    "\n",
    "  # training \n",
    "  for e in range(1000):\n",
    "    np.random.seed(100)\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        # pre-train data\n",
    "        e_loss += net.train_on_batch([ip[ind] for ip in fore_train_ip], fore_train_op[ind])\n",
    "        pbar.set_description('%f'%(e_loss/(start+1)))\n",
    "    \n",
    "    # validate at end of epoch\n",
    "    val_loss = net.evaluate(fore_valid_ip, fore_valid_op, batch_size=batch_size, verbose=1)\n",
    "    print ('Epoch', e, 'loss', e_loss*batch_size/samples_per_epoch, 'val loss', val_loss)\n",
    "    #train_losses.append(e_loss*batch_size/samples_per_epoch)\n",
    "    #val_losses.append(val_loss)\n",
    "    \n",
    "    # save best checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved new best checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "        best_epoch = e\n",
    "    \n",
    "      # save train and val losses for visualization\n",
    "    if train_FILE_PATH.exists():\n",
    "      with open(train_FILE_PATH, 'a') as lo:\n",
    "          reader = csv.writer(lo)\n",
    "          reader.writerow([str(e_loss*batch_size/samples_per_epoch)])\n",
    "      with open(val_FILE_PATH, 'a') as val_lo:\n",
    "              reader = csv.writer(val_lo)\n",
    "              reader.writerow([val_loss])\n",
    "\n",
    "    if not train_FILE_PATH.exists():\n",
    "        with open(train_FILE_PATH, 'w') as lo:\n",
    "            reader = csv.writer(lo)\n",
    "            reader.writerow([e_loss*batch_size/samples_per_epoch])\n",
    "        with open(val_FILE_PATH, 'w') as val_lo:\n",
    "            reader = csv.writer(val_lo)\n",
    "            reader.writerow([val_loss])  \n",
    "    \n",
    "    ckpt.step.assign_add(1)\n",
    "\n",
    "    if (e-best_epoch)>patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12371 [00:00<?, ?it/s]2024-03-23 15:17:03.330634: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1535e80d5330 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-23 15:17:03.330669: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2024-03-23 15:17:03.335738: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-23 15:17:03.703032: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2024-03-23 15:17:03.796295: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "0.209229:  93%|█████████▎| 11523/12371 [08:57<00:48, 17.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3696/3696 [==============================] - 61s 16ms/step - loss: 6.2130\n",
      "Epoch 0 loss 6.63968187382987 val loss 6.2129902839660645\n",
      "Saved new best checkpoint for step 1: ./sentence_bert_STraTS_20-124_masked_transformer_predsize_1V2/ckpt-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.179497: 100%|██████████| 12371/12371 [09:22<00:00, 21.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3696/3696 [==============================] - 61s 16ms/step - loss: 5.8627\n",
      "Epoch 1 loss 5.743760733344842 val loss 5.862745761871338\n",
      "Saved new best checkpoint for step 2: ./sentence_bert_STraTS_20-124_masked_transformer_predsize_1V2/ckpt-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.169943: 100%|██████████| 12371/12371 [09:21<00:00, 22.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3696/3696 [==============================] - 60s 16ms/step - loss: 5.5914\n",
      "Epoch 2 loss 5.438046396613738 val loss 5.591403961181641\n",
      "Saved new best checkpoint for step 3: ./sentence_bert_STraTS_20-124_masked_transformer_predsize_1V2/ckpt-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.164330: 100%|██████████| 12371/12371 [09:21<00:00, 22.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3696/3696 [==============================] - 60s 16ms/step - loss: 5.4886\n",
      "Epoch 3 loss 5.2584352629886695 val loss 5.4886393547058105\n",
      "Saved new best checkpoint for step 4: ./sentence_bert_STraTS_20-124_masked_transformer_predsize_1V2/ckpt-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.160931: 100%|██████████| 12371/12371 [09:21<00:00, 22.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3696/3696 [==============================] - 61s 16ms/step - loss: 5.3801\n",
      "Epoch 4 loss 5.149690664305115 val loss 5.380105018615723\n",
      "Saved new best checkpoint for step 5: ./sentence_bert_STraTS_20-124_masked_transformer_predsize_1V2/ckpt-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.158213: 100%|██████████| 12371/12371 [09:21<00:00, 22.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3696/3696 [==============================] - 60s 16ms/step - loss: 5.2985\n",
      "Epoch 5 loss 5.0627011319057225 val loss 5.298466682434082\n",
      "Saved new best checkpoint for step 6: ./sentence_bert_STraTS_20-124_masked_transformer_predsize_1V2/ckpt-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.155947: 100%|██████████| 12371/12371 [09:30<00:00, 21.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3696/3696 [==============================] - 61s 17ms/step - loss: 5.2386\n",
      "Epoch 6 loss 4.990175031541769 val loss 5.238565921783447\n",
      "Saved new best checkpoint for step 7: ./sentence_bert_STraTS_20-124_masked_transformer_predsize_1V2/ckpt-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.153996: 100%|██████████| 12371/12371 [09:21<00:00, 22.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3696/3696 [==============================] - 60s 16ms/step - loss: 5.1633\n",
      "Epoch 7 loss 4.927766759201962 val loss 5.163273334503174\n",
      "Saved new best checkpoint for step 8: ./sentence_bert_STraTS_20-124_masked_transformer_predsize_1V2/ckpt-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.152718: 100%|██████████| 12371/12371 [09:20<00:00, 22.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3696/3696 [==============================] - 60s 16ms/step - loss: 5.1379\n",
      "Epoch 8 loss 4.886874525930991 val loss 5.137864589691162\n",
      "Saved new best checkpoint for step 9: ./sentence_bert_STraTS_20-124_masked_transformer_predsize_1V2/ckpt-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.151546: 100%|██████████| 12371/12371 [09:26<00:00, 21.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2538/3696 [===================>..........] - ETA: 18s - loss: 5.2373"
     ]
    }
   ],
   "source": [
    "# training\n",
    "train_and_checkpoint(fore_model, manager)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
