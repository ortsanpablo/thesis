{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.metrics import classification_report as report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments 3, Forecast Run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nchest \\t0.033754\\ncare \\t0.032076\\nurine \\t0.030525\\nday \\t0.030009\\nfinal \\t0.029927\\nlung \\t0.028957\\nseen \\t0.028083\\npatient 0.027310\\nreport \\t0.027091\\nyear \\t0.026016\\ndaily \\t0.025909\\nhct \\t0.024847\\ntablet \\t0.024462\\nblood \\t0.023880\\nrespiratory \\t0.023798\\nexamination \\t0.023738\\nright \\t0.023121\\nicu \\t0.022048\\nvalve \\t0.021717\\nradiology \\t0.020575\\nline \\t0.020083\\nmedical \\t0.019867\\nmeq \\t0.019810\\nclear \\t0.019440\\nstatus \\t0.019176\\nplan \\t0.019171\\nresponse \\t0.019155\\nacute \\t0.018978\\nclip \\t0.018692\\npresent \\t0.018413\\nstable \\t0.018383\\nhospital \\t0.018349\\nhistory \\t0.018278\\nnumber \\t0.017874\\ntube \\t0.017609\\nsmall \\t0.017253\\nold \\t0.017168\\nnormal \\t0.017150\\nfluid \\t0.016643\\npulmonary \\t0.016506\\npain \\t0.015255\\nassessment \\t0.015216\\nfailure \\t0.014833\\nnoted \\t0.011953\\nleft \\t0.011104\\ncontinue \\t0.009939\\ncontrast \\t0.008908\\nreason \\t0.008502\\naction \\t0.006570\\ngiven \\t0.001878\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf features\n",
    "#tfidf_words = ['left', 'hospital', 'right', 'small', 'blood', 'patient', 'daily', 'assessment', 'clear', 'noted', 'action', 'pulmonary', 'response', 'plan', 'pain', 'stable', 'hct', 'fluid', 'meq', 'urine', 'continue', 'chest', 'clip', 'number', 'radiology', 'reason', 'medical', 'year', 'old', 'examination', 'final', 'report', 'day', 'status', 'present', 'normal', 'tube', 'acute', 'respiratory', 'failure', 'care', 'history', 'icu', 'tablet', 'lung', 'given', 'seen', 'line', 'contrast', 'valve']\n",
    "#tfidf_words\n",
    "\"\"\"\n",
    "chest \t0.033754\n",
    "care \t0.032076\n",
    "urine \t0.030525\n",
    "day \t0.030009\n",
    "final \t0.029927\n",
    "lung \t0.028957\n",
    "seen \t0.028083\n",
    "patient 0.027310\n",
    "report \t0.027091\n",
    "year \t0.026016\n",
    "daily \t0.025909\n",
    "hct \t0.024847\n",
    "tablet \t0.024462\n",
    "blood \t0.023880\n",
    "respiratory \t0.023798\n",
    "examination \t0.023738\n",
    "right \t0.023121\n",
    "icu \t0.022048\n",
    "valve \t0.021717\n",
    "radiology \t0.020575\n",
    "line \t0.020083\n",
    "medical \t0.019867\n",
    "meq \t0.019810\n",
    "clear \t0.019440\n",
    "status \t0.019176\n",
    "plan \t0.019171\n",
    "response \t0.019155\n",
    "acute \t0.018978\n",
    "clip \t0.018692\n",
    "present \t0.018413\n",
    "stable \t0.018383\n",
    "hospital \t0.018349\n",
    "history \t0.018278\n",
    "number \t0.017874\n",
    "tube \t0.017609\n",
    "small \t0.017253\n",
    "old \t0.017168\n",
    "normal \t0.017150\n",
    "fluid \t0.016643\n",
    "pulmonary \t0.016506\n",
    "pain \t0.015255\n",
    "assessment \t0.015216\n",
    "failure \t0.014833\n",
    "noted \t0.011953\n",
    "left \t0.011104\n",
    "continue \t0.009939\n",
    "contrast \t0.008908\n",
    "reason \t0.008502\n",
    "action \t0.006570\n",
    "given \t0.001878\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data, read comments to generate / load from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading STraTS-notext data and forecast and STraTS-SB IDs to get reference IDs that are in notext and text\n",
      "loading STraTS-notext data and forecast\n",
      "loading Sentence Bert STraTS data and forecast.\n",
      "loading TF-IDF STraTS data and forecast.\n"
     ]
    }
   ],
   "source": [
    "#lab data, extracted from mimic, preprocessed\n",
    "#forecasts from strats model, trained on train data, and then tasked to forecast test patients that it did never encounter during training. It got 4 hours of input and forecast 12 next hours.\n",
    "print(\"loading STraTS-notext data and forecast and STraTS-SB IDs to get reference IDs that are in notext and text\")\n",
    "no_text_forecasts = pd.read_pickle(\"Experiments/unseeded_models/forecasts/EXP3_STraTS_BASE_forecast.pkl\")\n",
    "notext_forecasts = no_text_forecasts[0]\n",
    "notext_var_map = list(no_text_forecasts[1].keys())\n",
    "test_IDs = list(notext_forecasts[\"ts_ind\"])\n",
    "del no_text_forecasts\n",
    "\n",
    "s_b_forecasts = pd.read_pickle(\"Experiments/unseeded_models/forecasts/EXP3_STraTS_SBERT_forecast.pkl\")\n",
    "sb_forecasts = s_b_forecasts[0]\n",
    "sb_var_map = list(s_b_forecasts[1].keys())\n",
    "sb_test_IDs = list(sb_forecasts[\"ts_ind\"])\n",
    "\n",
    "del s_b_forecasts\n",
    "\n",
    "full_ids = set(test_IDs) & set(sb_test_IDs)\n",
    "\n",
    "print(\"loading STraTS-notext data and forecast\")\n",
    "with open(\"./data/mimic_notext.pkl\", \"rb\") as pfile:\n",
    "    raw_data = pickle.load(pfile)\n",
    "\n",
    "## run following rows once \n",
    "## then comment out to generate data in correct format for evaluation, takes long\n",
    "#notext_mimic = raw_data[0]\n",
    "#notext_mimic = notext_mimic.loc[notext_mimic[\"ts_ind\"].isin(test_IDs)]\n",
    "#notext = notext_mimic.pivot_table(index=['ts_ind', \"hour\"], columns='variable', values='value')\n",
    "#notext = notext.reset_index()\n",
    "#print(notext)\n",
    "#notext.to_pickle(\"mimic_notext_for_eval.pkl\")\n",
    "\n",
    "#uncomment following line if you have generated the data already\n",
    "notext = pd.read_pickle(\"./data/mimic_notext_for_eval.pkl\")\n",
    "\n",
    "#notext_meta = raw_data[1]\n",
    "#notext_oc = notext_meta\n",
    "\n",
    "del raw_data\n",
    "\n",
    "\n",
    "## Sentence Bert\n",
    "print(\"loading Sentence Bert STraTS data and forecast.\")\n",
    "\n",
    "#s_b_forecasts = pd.read_pickle(\"Experiments/unseeded_models/forecasts/EXP3_STraTS_SBERT_forecast.pkl\")\n",
    "#sb_forecasts = s_b_forecasts[0]\n",
    "#sb_var_map = list(s_b_forecasts[1].keys())\n",
    "#sb_test_IDs = list(sb_forecasts[\"ts_ind\"])\n",
    "\n",
    "#del s_b_forecasts\n",
    "with open(\"./data/mimic_and_sbert_for_thesis.pkl\", \"rb\") as pfile:\n",
    "    sb_raw_data = pickle.load(pfile)\n",
    "\n",
    "\n",
    "## run following rows once \n",
    "# then comment out to generate data in correct format for evaluation, takes long\n",
    "#sb_mimic = sb_raw_data[0]\n",
    "#sb_mimic = sb_mimic.loc[sb_mimic[\"ts_ind\"].isin(sb_test_IDs)]\n",
    "#print(len(sb_mimic))\n",
    "#sb = sb_mimic.pivot_table(index=['ts_ind', \"hour\"], columns='variable', values='value')\n",
    "#sb = sb.reset_index()\n",
    "#sb.to_pickle(\"mimic_and_sbert_for_eval.pkl\")\n",
    "\n",
    "#uncomment following line if you have generated the data already\n",
    "sb = pd.read_pickle(\"./data/mimic_and_sbert_for_eval.pkl\")\n",
    "\n",
    "#sb_meta = sb_raw_data[1]\n",
    "#sb_oc = sb_meta\n",
    "\n",
    "del sb_raw_data\n",
    "\n",
    "# TF-IDF\n",
    "print(\"loading TF-IDF STraTS data and forecast.\")\n",
    "t_f_forecasts = pd.read_pickle(\"Experiments/unseeded_models/forecasts/EXP3_STraTS_TF-IDF_forecast.pkl\")\n",
    "tf_forecasts = t_f_forecasts[0]\n",
    "tf_var_map = list(t_f_forecasts[1].keys())\n",
    "tf_test_IDs = list(tf_forecasts[\"ts_ind\"])\n",
    "del t_f_forecasts\n",
    "\n",
    "with open(\"./data/mimic_and_tfidf_for_thesis.pkl\", \"rb\") as pfile:\n",
    "    tf_raw_data = pickle.load(pfile)\n",
    "\n",
    "\n",
    "## run following rows once \n",
    "# then comment out to generate data in correct format for evaluation, takes long\n",
    "#tf_mimic = tf_raw_data[0]\n",
    "#tf_mimic = tf_mimic.loc[tf_mimic[\"ts_ind\"].isin(tf_test_IDs)]\n",
    "#tf = tf_mimic.pivot_table(index=['ts_ind', \"hour\"], columns='variable', values='value')\n",
    "#tf = tf.reset_index()\n",
    "#tf.to_pickle(\"mimic_and_tfidf_for_eval.pkl\")\n",
    "\n",
    "#tf_meta = tf_raw_data[1]\n",
    "#tf_oc = tf_meta\n",
    "\n",
    "del tf_raw_data\n",
    "\n",
    "#uncomment following line if you have generated the data already\n",
    "tf = pd.read_pickle(\"./data/mimic_and_tfidf_for_eval.pkl\")\n",
    "\n",
    "del test_IDs, sb_test_IDs, tf_test_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_obs(df, mapping):\n",
    "    # Convert 'hour' to timedelta for proper grouping\n",
    "    df[\"hour\"] = df[\"hour\"].sub(4)\n",
    "    df['hour'] = pd.to_timedelta(df['hour'], unit='h')    \n",
    "    # Group by time step and calculate the mean for each hour\n",
    "    grouped = df.groupby([\"ts_ind\", pd.Grouper(key='hour', freq='1h')])[mapping].mean().reset_index()\n",
    "    grouped[\"hour\"] = grouped['hour'].dt.floor('H')\n",
    "    grouped[\"hour\"] = grouped.hour.astype(str).str.replace('0 days ', '')    \n",
    "    # Create a complete set of expected hours\n",
    "    expected_hours = pd.date_range(start='00:00:00', end='23:00:00', freq='1H').strftime('%H:%M:%S')\n",
    "\n",
    "    # Create a DataFrame with all combinations of ts_ind and expected hours\n",
    "    all_combinations = pd.DataFrame([(i, h) for i in grouped['ts_ind'].unique() for h in expected_hours], columns=['ts_ind', 'hour'])\n",
    "    # Merge the calculated means with the DataFrame of all combinations\n",
    "    grouped_filled = pd.merge(all_combinations, grouped, on=['ts_ind', 'hour'], how='left')\n",
    "\n",
    "    return grouped_filled\n",
    "\n",
    "def restore_predictions(predictions, mapping):\n",
    "    l = []\n",
    "    leng = len(predictions.iloc[0]) / len(mapping)\n",
    "    for i in range(len(predictions)):\n",
    "        arr = np.asarray(predictions.iloc[i]).reshape((int(leng),len(mapping)))\n",
    "        l.append(arr)\n",
    "    df = pd.concat([pd.DataFrame(arr) for arr in l])#, keys=np.arange(len(l)))\n",
    "    \n",
    "    df.columns=mapping\n",
    "    return df\n",
    "\n",
    "def mse_loss_with_nans(prediction, target, ax):\n",
    "\n",
    "    # Mask missing values in target observations\n",
    "    mask = pd.isna(target)\n",
    "\n",
    "    # mean of the squared error along axis. \n",
    "    # axis 1 is column wise e.g. for each feature\n",
    "    # axis 0 is row wise e.g. to get mse per timestep\n",
    "    mse = np.mean((target[~mask]-prediction[~mask])**2, axis=ax)\n",
    "    #mse = np.sum((target[~mask]-prediction)**2, axis=ax)\n",
    "\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 6]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,6,3,4,123]\n",
    "b = [5,1,2,6,3,4]\n",
    "list(set(a) & set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## NOTEXT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rm/v29286yx225c2ktvpkpwfr5r0000gq/T/ipykernel_41850/1321755664.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"hour\"] = df[\"hour\"].sub(4)\n",
      "/var/folders/rm/v29286yx225c2ktvpkpwfr5r0000gq/T/ipykernel_41850/1321755664.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['hour'] = pd.to_timedelta(df['hour'], unit='h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## SENTENCE BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rm/v29286yx225c2ktvpkpwfr5r0000gq/T/ipykernel_41850/1321755664.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"hour\"] = df[\"hour\"].sub(4)\n",
      "/var/folders/rm/v29286yx225c2ktvpkpwfr5r0000gq/T/ipykernel_41850/1321755664.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['hour'] = pd.to_timedelta(df['hour'], unit='h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## TF-IDF\n",
      "This needs to be True True True! True True True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rm/v29286yx225c2ktvpkpwfr5r0000gq/T/ipykernel_41850/1321755664.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"hour\"] = df[\"hour\"].sub(4)\n",
      "/var/folders/rm/v29286yx225c2ktvpkpwfr5r0000gq/T/ipykernel_41850/1321755664.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['hour'] = pd.to_timedelta(df['hour'], unit='h')\n"
     ]
    }
   ],
   "source": [
    "#select ids that have data for necessary amount of steps\n",
    "observed_steps = 4\n",
    "forecast_steps = 24\n",
    "\n",
    "print(\"## NOTEXT\")\n",
    "notext_IDS = list(np.unique(notext[\"ts_ind\"].loc[notext[\"hour\"] >= forecast_steps+observed_steps]))\n",
    "notext_fo_IDs = notext_forecasts[\"ts_ind\"].loc[notext_forecasts[\"ts_ind\"].isin(notext_IDS)]\n",
    "notext_fo_IDs = list(set(notext_fo_IDs) & set(full_ids))\n",
    "#select observed data based on the ids\n",
    "notext_observe = notext.loc[notext[\"ts_ind\"].isin(notext_fo_IDs)]\n",
    "#select gold values of the steps that were forecast\n",
    "notext_observe_targets_ = notext_observe[(notext_observe['hour'] > observed_steps) & (notext_observe['hour'] <= observed_steps+forecast_steps)]\n",
    "notext_observe_targets = build_obs(notext_observe_targets_, notext_var_map)\n",
    "#bring forecasts in correct format\n",
    "notext_fo = restore_predictions(notext_forecasts[\"forecasting_pred\"].loc[notext_forecasts[\"ts_ind\"].isin(notext_IDS)], notext_var_map)\n",
    "#bring observed in correct format\n",
    "notext_observe_targets = notext_observe_targets.drop(columns=[\"ts_ind\", \"hour\"])\n",
    "notext_fo = notext_fo.reset_index().drop(columns=\"index\")\n",
    "#compute MSE: for each step, Mean for all variables, of the squared difference between forecast and target\n",
    "notext_MSE = mse_loss_with_nans(notext_fo, notext_observe_targets,1)\n",
    "notext_step = list(range(1,25,1)) * len(notext_fo_IDs)\n",
    "notext_st = pd.DataFrame(notext_step, columns=[\"step\"])\n",
    "notext_st = notext_st.reset_index().drop(columns=\"index\")\n",
    "notext_st[\"mse\"] = notext_MSE\n",
    "notext_st.to_pickle(\"notext_st.pkl\")\n",
    "\n",
    "del notext_IDS, notext_observe, notext_observe_targets_, notext_MSE, notext_step\n",
    "\n",
    "print(\"## SENTENCE BERT\")\n",
    "sb_IDS = list(np.unique(sb[\"ts_ind\"].loc[sb[\"hour\"] >= forecast_steps+observed_steps]))\n",
    "sb_fo_IDs = sb_forecasts[\"ts_ind\"].loc[sb_forecasts[\"ts_ind\"].isin(sb_IDS)]\n",
    "sb_fo_IDs = list(set(sb_fo_IDs) & set(notext_fo_IDs))\n",
    "#select observed data based on the ids\n",
    "sb_observe = sb.loc[sb[\"ts_ind\"].isin(sb_fo_IDs)]\n",
    "#select gold values of the steps that were forecast \n",
    "sb_observe_targets_ = sb_observe[(sb_observe['hour'] > observed_steps) & (sb_observe['hour'] <= observed_steps+forecast_steps)]\n",
    "sb_observe_targets = build_obs(sb_observe_targets_, sb_var_map)\n",
    "#bring forecasts in correct format\n",
    "sb_fo = restore_predictions(sb_forecasts[\"forecasting_pred\"].loc[sb_forecasts[\"ts_ind\"].isin(sb_IDS)], sb_var_map)\n",
    "#bring observed in correct format\n",
    "sb_observe_targets = sb_observe_targets.drop(columns=[\"ts_ind\", \"hour\"])\n",
    "sb_fo = sb_fo.reset_index().drop(columns=\"index\")\n",
    "#compute MSE: for each step, Mean for all variables, of the squared difference between forecast and target\n",
    "sb_MSE = mse_loss_with_nans(sb_fo, sb_observe_targets,1)\n",
    "sb_step = list(range(1,25,1)) * len(sb_fo_IDs)\n",
    "sb_st = pd.DataFrame(sb_step, columns=[\"step\"])\n",
    "sb_st = sb_st.reset_index().drop(columns=\"index\")\n",
    "sb_st[\"mse\"] = sb_MSE\n",
    "sb_st.to_pickle(\"sb_st.pkl\")\n",
    "\n",
    "\n",
    "del sb_IDS, sb_observe, sb_observe_targets_, sb_MSE, sb_step\n",
    "\n",
    "\n",
    "print(\"## TF-IDF\")\n",
    "tf_IDS = list(np.unique(tf[\"ts_ind\"].loc[tf[\"hour\"] >= forecast_steps+observed_steps]))\n",
    "tf_fo_IDs = tf_forecasts[\"ts_ind\"].loc[tf_forecasts[\"ts_ind\"].isin(tf_IDS)]\n",
    "tf_fo_IDs = list(set(tf_fo_IDs) & set(notext_fo_IDs))\n",
    "compare = lambda x, y: Counter(x) == Counter(y)\n",
    "print(\"This needs to be True True True!\", compare(notext_fo_IDs, notext_fo_IDs), compare(notext_fo_IDs, tf_fo_IDs), compare(sb_fo_IDs, tf_fo_IDs))\n",
    "\n",
    "#select observed data based on the ids\n",
    "tf_observe = tf.loc[tf[\"ts_ind\"].isin(tf_fo_IDs)]\n",
    "#select gold values of the steps that were forecast \n",
    "tf_observe_targets_ = tf_observe[(tf_observe['hour'] > observed_steps) & (tf_observe['hour'] <= observed_steps+forecast_steps)]\n",
    "tf_observe_targets = build_obs(tf_observe_targets_, tf_var_map)\n",
    "#bring forecasts in correct format\n",
    "tf_fo = restore_predictions(tf_forecasts[\"forecasting_pred\"].loc[tf_forecasts[\"ts_ind\"].isin(tf_IDS)], tf_var_map)\n",
    "#bring observed in correct format\n",
    "tf_observe_targets = tf_observe_targets.drop(columns=[\"ts_ind\", \"hour\"])\n",
    "tf_fo = tf_fo.reset_index().drop(columns=\"index\")\n",
    "#compute MSE: for each step, Mean for all variables, of the squared difference between forecast and target\n",
    "tf_MSE = mse_loss_with_nans(tf_fo, tf_observe_targets,1)\n",
    "tf_step = list(range(1,25,1)) * len(tf_fo_IDs)\n",
    "tf_st = pd.DataFrame(tf_step, columns=[\"step\"])\n",
    "tf_st = tf_st.reset_index().drop(columns=\"index\")\n",
    "tf_st[\"mse\"] = tf_MSE\n",
    "tf_st.to_pickle(\"tf_st.pkl\")\n",
    "\n",
    "del notext_fo_IDs, sb_fo_IDs, tf_IDS, tf_fo_IDs, tf_observe, tf_observe_targets_, tf_MSE, tf_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([notext_st, notext_fo, notext_observe_targets, notext_var_map], open('idfix_Unseeded6_notext_st_fo_targets_dump.pkl','wb'))\n",
    "pickle.dump([sb_st, sb_fo, sb_observe_targets, sb_var_map], open('idfix_Unseeded6_sb_st_fo_targets_dump.pkl','wb'))\n",
    "pickle.dump([tf_st, tf_fo, tf_observe_targets, tf_var_map], open('idfix_Unseeded6_tf_st_fo_targets_dump.pkl','wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microsoft_sepsis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
