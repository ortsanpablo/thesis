{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 13:14:11.995335: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-22 13:14:12.582255: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-22 13:14:12.582294: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-22 13:14:12.582324: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-22 13:14:12.601623: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-22 13:14:16.351507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "# from tensorflow.keras import models\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "\n",
    "tf.keras.utils.set_random_seed(1)\n",
    "random.seed(100)\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport zipfile\\nwith zipfile.ZipFile('./data/Archiv.zip', 'r') as zip_ref:\\n    zip_ref.extractall('')\\n    \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import zipfile\n",
    "with zipfile.ZipFile('./data/Archiv.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/mimic_notext.pkl\", \"rb\") as pfile:\n",
    "    raw_data = pickle.load(pfile)\n",
    "mimic = raw_data[0]\n",
    "meta = raw_data[1]\n",
    "train_ind = raw_data[2]\n",
    "valid_ind = raw_data[3]\n",
    "test_ind = raw_data[4]\n",
    "data = mimic\n",
    "oc = meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_ind</th>\n",
       "      <th>hour</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "      <th>TABLE</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>988215</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Age</td>\n",
       "      <td>66.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>74.449104</td>\n",
       "      <td>54.324803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988216</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Gender</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.435381</td>\n",
       "      <td>0.495812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988217</th>\n",
       "      <td>0</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>DBP</td>\n",
       "      <td>-0.571963</td>\n",
       "      <td>chart</td>\n",
       "      <td>60.381199</td>\n",
       "      <td>14.653406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988218</th>\n",
       "      <td>0</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>GCS_eye</td>\n",
       "      <td>0.679176</td>\n",
       "      <td>chart</td>\n",
       "      <td>3.27426</td>\n",
       "      <td>1.068559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988219</th>\n",
       "      <td>0</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>GCS_motor</td>\n",
       "      <td>0.515222</td>\n",
       "      <td>chart</td>\n",
       "      <td>5.271332</td>\n",
       "      <td>1.414280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74780605</th>\n",
       "      <td>49404</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>MBP</td>\n",
       "      <td>0.152084</td>\n",
       "      <td>chart</td>\n",
       "      <td>79.406058</td>\n",
       "      <td>17.055932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74780606</th>\n",
       "      <td>49404</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>O2 Saturation</td>\n",
       "      <td>-0.639678</td>\n",
       "      <td>chart</td>\n",
       "      <td>96.833354</td>\n",
       "      <td>4.429348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74780607</th>\n",
       "      <td>49404</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>RR</td>\n",
       "      <td>1.473732</td>\n",
       "      <td>chart</td>\n",
       "      <td>19.555516</td>\n",
       "      <td>6.408548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74780608</th>\n",
       "      <td>49404</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>SBP</td>\n",
       "      <td>-0.489288</td>\n",
       "      <td>chart</td>\n",
       "      <td>121.726318</td>\n",
       "      <td>23.966085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74780609</th>\n",
       "      <td>49404</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>Urine</td>\n",
       "      <td>-0.267503</td>\n",
       "      <td>output</td>\n",
       "      <td>127.020097</td>\n",
       "      <td>138.391352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73792395 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ts_ind       hour       variable     value   TABLE        mean  \\\n",
       "988215         0   0.000000            Age      66.0     N/A   74.449104   \n",
       "988216         0   0.000000         Gender       1.0     N/A    0.435381   \n",
       "988217         0   0.033333            DBP -0.571963   chart   60.381199   \n",
       "988218         0   0.033333        GCS_eye  0.679176   chart     3.27426   \n",
       "988219         0   0.033333      GCS_motor  0.515222   chart    5.271332   \n",
       "...          ...        ...            ...       ...     ...         ...   \n",
       "74780605   49404  20.400000            MBP  0.152084   chart   79.406058   \n",
       "74780606   49404  20.400000  O2 Saturation -0.639678   chart   96.833354   \n",
       "74780607   49404  20.400000             RR  1.473732   chart   19.555516   \n",
       "74780608   49404  20.400000            SBP -0.489288   chart  121.726318   \n",
       "74780609   49404  20.400000          Urine -0.267503  output  127.020097   \n",
       "\n",
       "                 std  \n",
       "988215     54.324803  \n",
       "988216      0.495812  \n",
       "988217     14.653406  \n",
       "988218      1.068559  \n",
       "988219      1.414280  \n",
       "...              ...  \n",
       "74780605   17.055932  \n",
       "74780606    4.429348  \n",
       "74780607    6.408548  \n",
       "74780608   23.966085  \n",
       "74780609  138.391352  \n",
       "\n",
       "[73792395 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = oc['SUBJECT_ID'].tolist()\n",
    "labels = oc['in_hospital_sepsis'].tolist()\n",
    "\n",
    "new_patient_ids = []\n",
    "new_labels = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "  # print(i)\n",
    "  if ids[i] in new_patient_ids:\n",
    "    continue\n",
    "  else:\n",
    "    new_patient_ids.append(ids[i])\n",
    "    new_labels.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 44153, 1: 5252})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# data ratio\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23587\n",
      "31708\n",
      "7371\n",
      "9894\n",
      "5897\n",
      "7803\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x, x_test, y, y_test = train_test_split(new_patient_ids, new_labels, test_size=0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "# train\n",
    "train_ind = []\n",
    "\n",
    "ts_ind = oc['ts_ind'].tolist()\n",
    "# ids = ids\n",
    "\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_train:\n",
    "    train_ind.append(ts_ind[i])\n",
    "\n",
    "# number of train patients\n",
    "print(len(x_train))\n",
    "# number of train instances\n",
    "print(len(train_ind))\n",
    "# to np.array\n",
    "train_ind = np.array(train_ind)\n",
    "\n",
    "test_ind = []\n",
    "\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_test:\n",
    "    test_ind.append(ts_ind[i])\n",
    "\n",
    "# number of test patients\n",
    "print(len(x_test))\n",
    "# number of test instances\n",
    "print(len(test_ind))\n",
    "# to np.array\n",
    "test_ind = np.array(test_ind)\n",
    "\n",
    "valid_ind = []\n",
    "\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_val:\n",
    "    valid_ind.append(ts_ind[i])\n",
    "\n",
    "# number of test patients\n",
    "print(len(x_val))\n",
    "# number of test instances\n",
    "print(len(valid_ind))\n",
    "# to np.array\n",
    "valid_ind = np.array(valid_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_window = 1 # hours that the output vector represents. 1 because i want to learn to predict 1 hour many times\n",
    "obs_windows = range(20, 124, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79022it [00:00, 707697.40it/s]\n",
      "100%|██████████| 26/26 [04:06<00:00,  9.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# Remove test patients.\n",
    "data = data.merge(oc[['ts_ind', 'SUBJECT_ID']], on='ts_ind', how='left')\n",
    "test_sub = oc.loc[oc.ts_ind.isin(test_ind)].SUBJECT_ID.unique()\n",
    "data = data.loc[~data.SUBJECT_ID.isin(test_sub)]\n",
    "oc = oc.loc[~oc.SUBJECT_ID.isin(test_sub)]\n",
    "data.drop(columns=['SUBJECT_ID'], inplace=True)\n",
    "# Fix age.\n",
    "data.loc[(data.variable=='Age')&(data.value>200), 'value'] = 91.4\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "N = data.ts_ind.max()+1\n",
    "demo = np.zeros((N, D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds==0)*1 + (stds!=0)*stds\n",
    "demo = (demo-means)/stds\n",
    "# Get variable indices.\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Find max_len.\n",
    "fore_max_len = 880\n",
    "# Get forecast inputs and outputs.\n",
    "fore_times_ip = []\n",
    "fore_values_ip = []\n",
    "fore_varis_ip = []\n",
    "fore_op = []\n",
    "fore_inds = []\n",
    "def f(x):\n",
    "    mask = [0 for i in range(V)]\n",
    "    values = [0 for i in range(V)]\n",
    "    for vv in x:\n",
    "        v = int(vv[0])-1\n",
    "        mask[v] = 1\n",
    "        values[v] = vv[1]\n",
    "    return values+mask\n",
    "def pad(x):\n",
    "    return x+[0]*(fore_max_len-len(x))\n",
    "for w in tqdm(obs_windows):\n",
    "    pred_data = data.loc[(data.hour>=w)&(data.hour<=w+pred_window)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value':'first'}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value':list}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data['vind_value'].apply(f)\n",
    "    obs_data = data.loc[(data.hour<w)&(data.hour>=w-24)]\n",
    "    obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "    obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "    obs_data = obs_data.groupby('ts_ind').agg({'vind':list, 'hour':list, 'value':list}).reset_index()\n",
    "    obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "    for col in ['vind', 'hour', 'value']:\n",
    "        obs_data[col] = obs_data[col].apply(pad)\n",
    "    fore_op.append(np.array(list(obs_data.vind_value)))\n",
    "    fore_inds.append(np.array(list(obs_data.ts_ind)))\n",
    "    fore_times_ip.append(np.array(list(obs_data.hour)))\n",
    "    fore_values_ip.append(np.array(list(obs_data.value)))\n",
    "    fore_varis_ip.append(np.array(list(obs_data.vind)))\n",
    "del data\n",
    "fore_times_ip = np.concatenate(fore_times_ip, axis=0)\n",
    "fore_values_ip = np.concatenate(fore_values_ip, axis=0)\n",
    "fore_varis_ip = np.concatenate(fore_varis_ip, axis=0)\n",
    "fore_op = np.concatenate(fore_op, axis=0)\n",
    "fore_inds = np.concatenate(fore_inds, axis=0)\n",
    "fore_demo = demo[fore_inds]\n",
    "# Get train and valid ts_ind for forecast task.\n",
    "train_sub = oc.loc[oc.ts_ind.isin(train_ind)].SUBJECT_ID.unique()\n",
    "valid_sub = oc.loc[oc.ts_ind.isin(valid_ind)].SUBJECT_ID.unique()\n",
    "rem_sub = oc.loc[~oc.SUBJECT_ID.isin(np.concatenate((train_ind, valid_ind)))].SUBJECT_ID.unique()\n",
    "bp = int(0.8*len(rem_sub))\n",
    "train_sub = np.concatenate((train_sub, rem_sub[:bp]))\n",
    "valid_sub = np.concatenate((valid_sub, rem_sub[bp:]))\n",
    "train_ind = oc.loc[oc.SUBJECT_ID.isin(train_sub)].ts_ind.unique() # Add remaining ts_ind s of train subjects.\n",
    "valid_ind = oc.loc[oc.SUBJECT_ID.isin(valid_sub)].ts_ind.unique() # Add remaining ts_ind s of train subjects.\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ind = np.argwhere(np.in1d(fore_inds, train_ind)).flatten()\n",
    "valid_ind = np.argwhere(np.in1d(fore_inds, valid_ind)).flatten()\n",
    "fore_train_ip = [ip[train_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "fore_valid_ip = [ip[valid_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "del fore_times_ip, fore_values_ip, fore_varis_ip, demo, fore_demo\n",
    "fore_train_op = fore_op[train_ind]\n",
    "fore_valid_op = fore_op[valid_ind]\n",
    "del fore_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndata = mimic\\noc = meta\\ndata = data.dropna()\\noc = oc.dropna()\\n\\n# train\\ntrain_ind = []\\nts_ind = oc['ts_ind'].tolist()\\n# ids = ids\\nfor i in range(len(ts_ind)):\\n  if ids[i] in x_train:\\n    train_ind.append(ts_ind[i])\\n# number of train patients\\nprint(len(x_train))\\n# number of train instances\\nprint(len(train_ind))\\n# to np.array\\ntrain_ind = np.array(train_ind)\\ntest_ind = []\\nfor i in range(len(ts_ind)):\\n  if ids[i] in x_test:\\n    test_ind.append(ts_ind[i])\\n# number of test patients\\nprint(len(x_test))\\n# number of test instances\\nprint(len(test_ind))\\n# to np.array\\ntest_ind = np.array(test_ind)\\nvalid_ind = []\\nfor i in range(len(ts_ind)):\\n  if ids[i] in x_val:\\n    valid_ind.append(ts_ind[i])\\n# number of test patients\\nprint(len(x_val))\\n# number of test instances\\nprint(len(valid_ind))\\n# to np.array\\nvalid_ind = np.array(valid_ind)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "data = mimic\n",
    "oc = meta\n",
    "data = data.dropna()\n",
    "oc = oc.dropna()\n",
    "\n",
    "# train\n",
    "train_ind = []\n",
    "ts_ind = oc['ts_ind'].tolist()\n",
    "# ids = ids\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_train:\n",
    "    train_ind.append(ts_ind[i])\n",
    "# number of train patients\n",
    "print(len(x_train))\n",
    "# number of train instances\n",
    "print(len(train_ind))\n",
    "# to np.array\n",
    "train_ind = np.array(train_ind)\n",
    "test_ind = []\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_test:\n",
    "    test_ind.append(ts_ind[i])\n",
    "# number of test patients\n",
    "print(len(x_test))\n",
    "# number of test instances\n",
    "print(len(test_ind))\n",
    "# to np.array\n",
    "test_ind = np.array(test_ind)\n",
    "valid_ind = []\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_val:\n",
    "    valid_ind.append(ts_ind[i])\n",
    "# number of test patients\n",
    "print(len(x_val))\n",
    "# number of test instances\n",
    "print(len(valid_ind))\n",
    "# to np.array\n",
    "valid_ind = np.array(valid_ind)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# # Read data.\\n# data_path = './../mimic_iii_preprocessed.pkl'\\n# data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\\n# Filter labeled data in first 24h.\\ndata = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\\ndata = data.loc[(data.hour>=0)&(data.hour<=24)]\\noc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\\n# Fix age.\\ndata.loc[(data.variable=='o:age')&(data.value>200), 'value'] = 91.4\\n# Get y and N.\\ny = np.array(oc.sort_values(by='ts_ind')['sepsis_label']).astype('float32')\\nN = data.ts_ind.max() + 1\\n# Get static data with mean fill and missingness indicator.\\nstatic_varis = ['o:age', 'o:gender']\\nii = data.variable.isin(static_varis)\\nstatic_data = data.loc[ii]\\ndata = data.loc[~ii]\\ndef inv_list(l, start=0):\\n    d = {}\\n    for i in range(len(l)):\\n        d[l[i]] = i+start\\n    return d\\nstatic_var_to_ind = inv_list(static_varis)\\nD = len(static_varis)\\ndemo = np.zeros((N, D))\\nfor row in tqdm(static_data.itertuples()):\\n    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\\n# Normalize static data.\\nmeans = demo.mean(axis=0, keepdims=True)\\nstds = demo.std(axis=0, keepdims=True)\\nstds = (stds==0)*1 + (stds!=0)*stds\\ndemo = (demo-means)/stds\\n# Trim to max len.\\n#data = data.sample(frac=1)\\ndata = data.groupby('ts_ind').head(880)\\n# Get N, V, var_to_ind.\\nN = data.ts_ind.max() + 1\\nvaris = sorted(list(set(data.variable)))\\nV = len(varis)\\ndef inv_list(l, start=0):\\n    d = {}\\n    for i in range(len(l)):\\n        d[l[i]] = i+start\\n    return d\\nvar_to_ind = inv_list(varis, start=1)\\ndata['vind'] = data.variable.map(var_to_ind)\\ndata = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\\n# Add obs index.\\ndata = data.sort_values(by=['ts_ind']).reset_index(drop=True)\\ndata = data.reset_index().rename(columns={'index':'obs_ind'})\\ndata = data.merge(data.groupby('ts_ind').agg({'obs_ind':'min'}).reset_index().rename(columns={                                                             'obs_ind':'first_obs_ind'}), on='ts_ind')\\ndata['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\\n# Find max_len.\\nmax_len = data.obs_ind.max()+1\\nprint ('max_len', max_len)\\n# Generate times_ip and values_ip matrices.\\ntimes_inp = np.zeros((N, max_len), dtype='float32')\\nvalues_inp = np.zeros((N, max_len), dtype='float32')\\nvaris_inp = np.zeros((N, max_len), dtype='int32')\\nfor row in tqdm(data.itertuples()):\\n    ts_ind = row.ts_ind\\n    l = row.obs_ind\\n    times_inp[ts_ind, l] = row.hour\\n    values_inp[ts_ind, l] = row.value\\n    varis_inp[ts_ind, l] = row.vind\\ndata.drop(columns=['obs_ind', 'first_obs_ind'], inplace=True)\\n# Generate 3 sets of inputs and outputs.\\ntrain_ip = [ip[train_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\\nvalid_ip = [ip[valid_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\\ntest_ip = [ip[test_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\\ndel times_inp, values_inp, varis_inp\\ntrain_op = y[train_ind]\\nvalid_op = y[valid_ind]\\ntest_op = y[test_ind]\\ndel y\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dont need data preperation for classification task\n",
    "\"\"\"\n",
    "# # Read data.\n",
    "# data_path = './../mimic_iii_preprocessed.pkl'\n",
    "# data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\n",
    "# Filter labeled data in first 24h.\n",
    "data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "data = data.loc[(data.hour>=0)&(data.hour<=24)]\n",
    "oc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# Fix age.\n",
    "data.loc[(data.variable=='o:age')&(data.value>200), 'value'] = 91.4\n",
    "# Get y and N.\n",
    "y = np.array(oc.sort_values(by='ts_ind')['sepsis_label']).astype('float32')\n",
    "N = data.ts_ind.max() + 1\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['o:age', 'o:gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "demo = np.zeros((N, D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds==0)*1 + (stds!=0)*stds\n",
    "demo = (demo-means)/stds\n",
    "# Trim to max len.\n",
    "#data = data.sample(frac=1)\n",
    "data = data.groupby('ts_ind').head(880)\n",
    "# Get N, V, var_to_ind.\n",
    "N = data.ts_ind.max() + 1\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Add obs index.\n",
    "data = data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "data = data.reset_index().rename(columns={'index':'obs_ind'})\n",
    "data = data.merge(data.groupby('ts_ind').agg({'obs_ind':'min'}).reset_index().rename(columns={ \\\n",
    "                                                            'obs_ind':'first_obs_ind'}), on='ts_ind')\n",
    "data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "# Find max_len.\n",
    "max_len = data.obs_ind.max()+1\n",
    "print ('max_len', max_len)\n",
    "# Generate times_ip and values_ip matrices.\n",
    "times_inp = np.zeros((N, max_len), dtype='float32')\n",
    "values_inp = np.zeros((N, max_len), dtype='float32')\n",
    "varis_inp = np.zeros((N, max_len), dtype='int32')\n",
    "for row in tqdm(data.itertuples()):\n",
    "    ts_ind = row.ts_ind\n",
    "    l = row.obs_ind\n",
    "    times_inp[ts_ind, l] = row.hour\n",
    "    values_inp[ts_ind, l] = row.value\n",
    "    varis_inp[ts_ind, l] = row.vind\n",
    "data.drop(columns=['obs_ind', 'first_obs_ind'], inplace=True)\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ip = [ip[train_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "valid_ip = [ip[valid_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "test_ip = [ip[test_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "del times_inp, values_inp, varis_inp\n",
    "train_op = y[train_ind]\n",
    "valid_op = y[valid_ind]\n",
    "test_op = y[test_ind]\n",
    "del y\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
    "def get_res(y_true, y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    minrp = np.minimum(precision, recall).max()\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    return [roc_auc, pr_auc, minrp]\n",
    "\n",
    "########################################################################################################\n",
    "########################################################################################################\n",
    "#class_weights = compute_class_weight(class_weight='balanced', classes=[0,1], y=train_op)\n",
    "def mortality_loss(y_true, y_pred):\n",
    "    sample_weights = (1-y_true)*class_weights[0] + y_true*class_weights[1]\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    return K.mean(sample_weights*bce, axis=-1)\n",
    "########################################################################################################\n",
    "########################################################################################################\n",
    "\n",
    "# var_weights = np.sum(fore_train_op[:, V:], axis=0)\n",
    "# var_weights[var_weights==0] = var_weights.max()\n",
    "# var_weights = var_weights.max()/var_weights\n",
    "# var_weights = var_weights.reshape((1, V))\n",
    "def forecast_loss(y_true, y_pred):\n",
    "    return K.sum(y_true[:,V:]*(y_true[:,:V]-y_pred)**2, axis=-1)\n",
    "\n",
    "def mape_fore(y_true, y_pred):\n",
    "    truth = y_true[:,V:]\n",
    "    pred = y_pred\n",
    "    return mape(truth, pred)\n",
    "\n",
    "                                          \n",
    "def get_min_loss(weight):\n",
    "    def min_loss(y_true, y_pred):\n",
    "        return weight*y_pred\n",
    "    return min_loss\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, validation_data, batch_size):\n",
    "        self.val_x, self.val_y = validation_data\n",
    "        self.batch_size = batch_size\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.val_x, verbose=0, batch_size=self.batch_size)\n",
    "        if type(y_pred)==type([]):\n",
    "            y_pred = y_pred[0]\n",
    "        precision, recall, thresholds = precision_recall_curve(self.val_y, y_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        roc_auc = roc_auc_score(self.val_y, y_pred)\n",
    "        logs['custom_metric'] = pr_auc + roc_auc\n",
    "        print ('val_aucs:', pr_auc, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Activation, Dropout, Softmax, Layer, InputSpec, Input, Dense, Lambda, TimeDistributed, Concatenate, Add\n",
    "from tensorflow.keras import initializers, regularizers, constraints, Model\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow import nn\n",
    "import smart_cond_mod as sc\n",
    "\n",
    "\n",
    "class CVE(Layer):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        super(CVE, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W1 = self.add_weight(name='CVE_W1',\n",
    "                            shape=(1, self.hid_units),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        self.b1 = self.add_weight(name='CVE_b1',\n",
    "                            shape=(self.hid_units,),\n",
    "                            initializer='zeros',\n",
    "                            trainable=True)\n",
    "        self.W2 = self.add_weight(name='CVE_W2',\n",
    "                            shape=(self.hid_units, self.output_dim),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        super(CVE, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = K.expand_dims(x, axis=-1)\n",
    "        x = K.dot(K.tanh(K.bias_add(K.dot(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape + (self.output_dim,)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, hid_dim):\n",
    "        self.hid_dim = hid_dim\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        self.W = self.add_weight(shape=(d, self.hid_dim), name='Att_W',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.hid_dim,), name='Att_b',\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(shape=(self.hid_dim,1), name='Att_u',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = K.dot(K.tanh(K.bias_add(K.dot(x,self.W), self.b)), self.u)\n",
    "        mask = K.expand_dims(mask, axis=-1)\n",
    "        attn_weights = mask*attn_weights + (1-mask)*mask_value\n",
    "        attn_weights = K.softmax(attn_weights, axis=-2)\n",
    "        return attn_weights\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (1,)\n",
    "\n",
    "\n",
    "class Transformer(Layer):\n",
    "\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0):\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = K.epsilon() * K.epsilon()\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        if self.dk==None:\n",
    "            self.dk = d//self.h\n",
    "        if self.dv==None:\n",
    "            self.dv = d//self.h\n",
    "        if self.dff==None:\n",
    "            self.dff = 2*d\n",
    "        self.Wq = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wq',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wk = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wk',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wv = self.add_weight(shape=(self.N, self.h, d, self.dv), name='Wv',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wo = self.add_weight(shape=(self.N, self.dv*self.h, d), name='Wo',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.W1 = self.add_weight(shape=(self.N, d, self.dff), name='W1',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b1 = self.add_weight(shape=(self.N, self.dff), name='b1',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.W2 = self.add_weight(shape=(self.N, self.dff, d), name='W2',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b2 = self.add_weight(shape=(self.N, d), name='b2',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.gamma = self.add_weight(shape=(2*self.N,), name='gamma',\n",
    "                                 initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(shape=(2*self.N,), name='beta',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Transformer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask, mask_value=-1e-30):\n",
    "        mask = K.expand_dims(mask, axis=-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = K.dot(x, self.Wq[i,j,:,:])\n",
    "                k = K.permute_dimensions(K.dot(x, self.Wk[i,j,:,:]), (0,2,1))\n",
    "                v = K.dot(x, self.Wv[i,j,:,:])\n",
    "                A = K.batch_dot(q,k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask*A + (1-mask)*mask_value\n",
    "                # Mask for attention dropout.\n",
    "                def dropped_A():\n",
    "                    dp_mask = K.cast((K.random_uniform(shape=array_ops.shape(A))>=self.dropout), K.floatx())\n",
    "                    return A*dp_mask + (1-dp_mask)*mask_value\n",
    "                A = sc.smart_cond(K.learning_phase(), dropped_A, lambda: array_ops.identity(A))\n",
    "                A = K.softmax(A, axis=-1)\n",
    "                mha_ops.append(K.batch_dot(A,v))\n",
    "            conc = K.concatenate(mha_ops, axis=-1)\n",
    "            proj = K.dot(conc, self.Wo[i,:,:])\n",
    "            # Dropout.\n",
    "            proj = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(proj, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(proj))\n",
    "            # Add & LN\n",
    "            x = x+proj\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i] + self.beta[2*i]\n",
    "            # FFN\n",
    "            ffn_op = K.bias_add(K.dot(K.relu(K.bias_add(K.dot(x, self.W1[i,:,:]), self.b1[i,:])),\n",
    "                           self.W2[i,:,:]), self.b2[i,:,])\n",
    "            # Dropout.\n",
    "            ffn_op = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(ffn_op, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(ffn_op))\n",
    "            # Add & LN\n",
    "            x = x+ffn_op\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i+1] + self.beta[2*i+1]\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "def build_strats(D, max_len, V, d, N, he, dropout, forecast=False):\n",
    "    demo = Input(shape=(D,))\n",
    "    demo_enc = Dense(2*d, activation='tanh')(demo)\n",
    "    demo_enc = Dense(d, activation='tanh')(demo_enc)\n",
    "    varis = Input(shape=(max_len,))\n",
    "    values = Input(shape=(max_len,))\n",
    "    times = Input(shape=(max_len,))\n",
    "    varis_emb = Embedding(V+1, d)(varis)\n",
    "    cve_units = int(np.sqrt(d))\n",
    "    values_emb = CVE(cve_units, d)(values)\n",
    "    times_emb = CVE(cve_units, d)(times)\n",
    "    comb_emb = Add()([varis_emb, values_emb, times_emb]) # b, L, d\n",
    "#     demo_enc = Lambda(lambda x:K.expand_dims(x, axis=-2))(demo_enc) # b, 1, d\n",
    "#     comb_emb = Concatenate(axis=-2)([demo_enc, comb_emb]) # b, L+1, d\n",
    "    mask = Lambda(lambda x:K.clip(x,0,1))(varis) # b, L\n",
    "#     mask = Lambda(lambda x:K.concatenate((K.ones_like(x)[:,0:1], x), axis=-1))(mask) # b, L+1\n",
    "    cont_emb = Transformer(N, he, dk=None, dv=None, dff=None, dropout=dropout)(comb_emb, mask=mask)\n",
    "    attn_weights = Attention(2*d)(cont_emb, mask=mask)\n",
    "    fused_emb = Lambda(lambda x:K.sum(x[0]*x[1], axis=-2))([cont_emb, attn_weights])\n",
    "    conc = Concatenate(axis=-1)([fused_emb, demo_enc])\n",
    "    fore_op = Dense(V)(conc)\n",
    "    op = Dense(1, activation='sigmoid')(fore_op)\n",
    "    model = Model([demo, times, values, varis], op)\n",
    "    if forecast:\n",
    "        fore_model = Model([demo, times, values, varis], fore_op)\n",
    "        return [model, fore_model]\n",
    "    return model\n",
    "\n",
    "# To tune:\n",
    "# 1. Transformer parameters. (N, h, dropout)\n",
    "# 2. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 13:20:13.996711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31132 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3a:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 880)]                0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 880)]                0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 880)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 880, 50)              6700      ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " cve (CVE)                   (None, 880, 50)              364       ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " cve_1 (CVE)                 (None, 880, 50)              364       ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 880, 50)              0         ['embedding[0][0]',           \n",
      "                                                                     'cve[0][0]',                 \n",
      "                                                                     'cve_1[0][0]']               \n",
      "                                                                                                  \n",
      " lambda (Lambda)             (None, 880)                  0         ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " transformer (Transformer)   (None, 880, 50)              39508     ['add[0][0]',                 \n",
      "                                                                     'lambda[0][0]']              \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)        [(None, 2)]                  0         []                            \n",
      "                                                                                                  \n",
      " attention (Attention)       (None, 880, 1)               5200      ['transformer[0][0]',         \n",
      "                                                                     'lambda[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 100)                  300       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)           (None, 50)                   0         ['transformer[0][0]',         \n",
      "                                                                     'attention[0][0]']           \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 50)                   5050      ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 100)                  0         ['lambda_1[0][0]',            \n",
      "                                                                     'dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 133)                  13433     ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 70919 (277.03 KB)\n",
      "Trainable params: 70919 (277.03 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "#surely can be done more elegant...\n",
    "fore_savepath = './STraTS_20-124_predsize_1V3'\n",
    "\n",
    "train_FILE_PATH = Path(f'{fore_savepath}/train_losses.csv')\n",
    "val_FILE_PATH = Path(f'{fore_savepath}/val_losses.csv')\n",
    "\n",
    "# initialize model parameters\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, len(fore_train_op), 5\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model, fore_model =  build_strats(D, fore_max_len, V, d, N, he, dropout, forecast=True)\n",
    "print(fore_model.summary())\n",
    "lossfunction = forecast_loss\n",
    "opt = tf.keras.optimizers.Adam(lr)\n",
    "fore_model.compile(loss=lossfunction, optimizer=opt)\n",
    "\n",
    "# initialize checkpoint manager\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=fore_model)\n",
    "manager = tf.train.CheckpointManager(ckpt, f'{fore_savepath}', max_to_keep=3)\n",
    "\n",
    "# define training procedure\n",
    "def train_and_checkpoint(net, manager):\n",
    "  # initialize loss, etc\n",
    "  best_val_loss = np.inf\n",
    "  N_fore = len(fore_train_op)\n",
    "  train_losses = []\n",
    "  val_losses = []\n",
    "\n",
    "  # load or create model\n",
    "  ckpt.restore(manager.latest_checkpoint)\n",
    "  if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "  else:\n",
    "    print(\"Initializing from scratch.\")\n",
    "\n",
    "  # training \n",
    "  for e in range(1000):\n",
    "    np.random.seed(100)\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        # pre-train data\n",
    "        e_loss += net.train_on_batch([ip[ind] for ip in fore_train_ip], fore_train_op[ind])\n",
    "        pbar.set_description('%f'%(e_loss/(start+1)))\n",
    "    \n",
    "    # validate at end of epoch\n",
    "    val_loss = net.evaluate(fore_valid_ip, fore_valid_op, batch_size=batch_size, verbose=1)\n",
    "    print ('Epoch', e, 'loss', e_loss*batch_size/samples_per_epoch, 'val loss', val_loss)\n",
    "    #train_losses.append(e_loss*batch_size/samples_per_epoch)\n",
    "    #val_losses.append(val_loss)\n",
    "    \n",
    "    # save best checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved new best checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "        best_epoch = e\n",
    "    \n",
    "      # save train and val losses for visualization\n",
    "    if train_FILE_PATH.exists():\n",
    "      with open(train_FILE_PATH, 'a') as lo:\n",
    "          reader = csv.writer(lo)\n",
    "          reader.writerow([str(e_loss*batch_size/samples_per_epoch)])\n",
    "      with open(val_FILE_PATH, 'a') as val_lo:\n",
    "              reader = csv.writer(val_lo)\n",
    "              reader.writerow([val_loss])\n",
    "\n",
    "    if not train_FILE_PATH.exists():\n",
    "        with open(train_FILE_PATH, 'w') as lo:\n",
    "            reader = csv.writer(lo)\n",
    "            reader.writerow([e_loss*batch_size/samples_per_epoch])\n",
    "        with open(val_FILE_PATH, 'w') as val_lo:\n",
    "            reader = csv.writer(val_lo)\n",
    "            reader.writerow([val_loss])  \n",
    "    \n",
    "    ckpt.step.assign_add(1)\n",
    "\n",
    "    if (e-best_epoch)>patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12323 [00:00<?, ?it/s]2024-03-22 13:20:19.944188: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x145e99a002c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-22 13:20:19.944216: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2024-03-22 13:20:19.949013: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-22 13:20:20.219620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2024-03-22 13:20:20.298623: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "0.226887:  52%|█████▏    | 6421/12323 [04:57<04:24, 22.30it/s] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "0.152026:  93%|█████████▎| 11412/12323 [08:41<00:41, 21.97it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "train_and_checkpoint(fore_model, manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
