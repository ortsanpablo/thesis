{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "XXs5pMOVDxPw",
    "outputId": "774cca6e-6f38-4e4a-ee73-dfd004340c62",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 01:41:14.445090: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-08 01:41:15.003631: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-08 01:41:15.003669: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-08 01:41:15.003701: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-08 01:41:15.025377: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-08 01:41:18.076699: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf. __version__)\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "# from tensorflow.keras import models\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#from google.colab import files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "#import smart_cond as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/mimic_and_tfidf_for_thesis.pkl\", \"rb\") as pfile:\n",
    "    raw_data = pickle.load(pfile)\n",
    "mimic = raw_data[0]\n",
    "meta = raw_data[1]\n",
    "train_ind = raw_data[2]\n",
    "valid_ind = raw_data[3]\n",
    "test_ind = raw_data[4]\n",
    "data = mimic\n",
    "oc = meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = oc['SUBJECT_ID'].tolist()\n",
    "labels = oc['in_hospital_sepsis'].tolist()\n",
    "\n",
    "new_patient_ids = []\n",
    "new_labels = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "  # print(i)\n",
    "  if ids[i] in new_patient_ids:\n",
    "    continue\n",
    "  else:\n",
    "    new_patient_ids.append(ids[i])\n",
    "    new_labels.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23587\n",
      "31708\n",
      "7371\n",
      "9894\n",
      "5897\n",
      "7803\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x, x_test, y, y_test = train_test_split(new_patient_ids, new_labels, test_size=0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "# train\n",
    "train_ind = []\n",
    "\n",
    "ts_ind = oc['ts_ind'].tolist()\n",
    "# ids = ids\n",
    "\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_train:\n",
    "    train_ind.append(ts_ind[i])\n",
    "\n",
    "# number of train patients\n",
    "print(len(x_train))\n",
    "# number of train instances\n",
    "print(len(train_ind))\n",
    "# to np.array\n",
    "train_ind = np.array(train_ind)\n",
    "\n",
    "test_ind = []\n",
    "\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_test:\n",
    "    test_ind.append(ts_ind[i])\n",
    "\n",
    "# number of test patients\n",
    "print(len(x_test))\n",
    "# number of test instances\n",
    "print(len(test_ind))\n",
    "# to np.array\n",
    "test_ind = np.array(test_ind)\n",
    "\n",
    "valid_ind = []\n",
    "\n",
    "for i in range(len(ts_ind)):\n",
    "  if ids[i] in x_val:\n",
    "    valid_ind.append(ts_ind[i])\n",
    "\n",
    "# number of test patients\n",
    "print(len(x_val))\n",
    "# number of test instances\n",
    "print(len(valid_ind))\n",
    "# to np.array\n",
    "valid_ind = np.array(valid_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# # Read data.\\n# data_path = './../mimic_iii_preprocessed.pkl'\\n# data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\\n# Filter labeled data in first 24h.\\ndata = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\\ndata = data.loc[(data.hour>=0)&(data.hour<=24)]\\noc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\\n# Fix age.\\ndata.loc[(data.variable=='o:age')&(data.value>200), 'value'] = 91.4\\n# Get y and N.\\ny = np.array(oc.sort_values(by='ts_ind')['sepsis_label']).astype('float32')\\nN = data.ts_ind.max() + 1\\n# Get static data with mean fill and missingness indicator.\\nstatic_varis = ['o:age', 'o:gender']\\nii = data.variable.isin(static_varis)\\nstatic_data = data.loc[ii]\\ndata = data.loc[~ii]\\ndef inv_list(l, start=0):\\n    d = {}\\n    for i in range(len(l)):\\n        d[l[i]] = i+start\\n    return d\\nstatic_var_to_ind = inv_list(static_varis)\\nD = len(static_varis)\\ndemo = np.zeros((N, D))\\nfor row in tqdm(static_data.itertuples()):\\n    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\\n# Normalize static data.\\nmeans = demo.mean(axis=0, keepdims=True)\\nstds = demo.std(axis=0, keepdims=True)\\nstds = (stds==0)*1 + (stds!=0)*stds\\ndemo = (demo-means)/stds\\n# Trim to max len.\\n#data = data.sample(frac=1)\\ndata = data.groupby('ts_ind').head(880)\\n# Get N, V, var_to_ind.\\nN = data.ts_ind.max() + 1\\nvaris = sorted(list(set(data.variable)))\\nV = len(varis)\\ndef inv_list(l, start=0):\\n    d = {}\\n    for i in range(len(l)):\\n        d[l[i]] = i+start\\n    return d\\nvar_to_ind = inv_list(varis, start=1)\\ndata['vind'] = data.variable.map(var_to_ind)\\ndata = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\\n# Add obs index.\\ndata = data.sort_values(by=['ts_ind']).reset_index(drop=True)\\ndata = data.reset_index().rename(columns={'index':'obs_ind'})\\ndata = data.merge(data.groupby('ts_ind').agg({'obs_ind':'min'}).reset_index().rename(columns={                                                             'obs_ind':'first_obs_ind'}), on='ts_ind')\\ndata['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\\n# Find max_len.\\nmax_len = data.obs_ind.max()+1\\nprint ('max_len', max_len)\\n# Generate times_ip and values_ip matrices.\\ntimes_inp = np.zeros((N, max_len), dtype='float32')\\nvalues_inp = np.zeros((N, max_len), dtype='float32')\\nvaris_inp = np.zeros((N, max_len), dtype='int32')\\nfor row in tqdm(data.itertuples()):\\n    ts_ind = row.ts_ind\\n    l = row.obs_ind\\n    times_inp[ts_ind, l] = row.hour\\n    values_inp[ts_ind, l] = row.value\\n    varis_inp[ts_ind, l] = row.vind\\ndata.drop(columns=['obs_ind', 'first_obs_ind'], inplace=True)\\n# Generate 3 sets of inputs and outputs.\\ntrain_ip = [ip[train_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\\nvalid_ip = [ip[valid_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\\ntest_ip = [ip[test_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\\ndel times_inp, values_inp, varis_inp\\ntrain_op = y[train_ind]\\nvalid_op = y[valid_ind]\\ntest_op = y[test_ind]\\ndel y\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dont need data preperation for classification task\n",
    "\"\"\"\n",
    "# # Read data.\n",
    "# data_path = './../mimic_iii_preprocessed.pkl'\n",
    "# data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\n",
    "# Filter labeled data in first 24h.\n",
    "data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "data = data.loc[(data.hour>=0)&(data.hour<=24)]\n",
    "oc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# Fix age.\n",
    "data.loc[(data.variable=='o:age')&(data.value>200), 'value'] = 91.4\n",
    "# Get y and N.\n",
    "y = np.array(oc.sort_values(by='ts_ind')['sepsis_label']).astype('float32')\n",
    "N = data.ts_ind.max() + 1\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['o:age', 'o:gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "demo = np.zeros((N, D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds==0)*1 + (stds!=0)*stds\n",
    "demo = (demo-means)/stds\n",
    "# Trim to max len.\n",
    "#data = data.sample(frac=1)\n",
    "data = data.groupby('ts_ind').head(880)\n",
    "# Get N, V, var_to_ind.\n",
    "N = data.ts_ind.max() + 1\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Add obs index.\n",
    "data = data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "data = data.reset_index().rename(columns={'index':'obs_ind'})\n",
    "data = data.merge(data.groupby('ts_ind').agg({'obs_ind':'min'}).reset_index().rename(columns={ \\\n",
    "                                                            'obs_ind':'first_obs_ind'}), on='ts_ind')\n",
    "data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "# Find max_len.\n",
    "max_len = data.obs_ind.max()+1\n",
    "print ('max_len', max_len)\n",
    "# Generate times_ip and values_ip matrices.\n",
    "times_inp = np.zeros((N, max_len), dtype='float32')\n",
    "values_inp = np.zeros((N, max_len), dtype='float32')\n",
    "varis_inp = np.zeros((N, max_len), dtype='int32')\n",
    "for row in tqdm(data.itertuples()):\n",
    "    ts_ind = row.ts_ind\n",
    "    l = row.obs_ind\n",
    "    times_inp[ts_ind, l] = row.hour\n",
    "    values_inp[ts_ind, l] = row.value\n",
    "    varis_inp[ts_ind, l] = row.vind\n",
    "data.drop(columns=['obs_ind', 'first_obs_ind'], inplace=True)\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ip = [ip[train_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "valid_ip = [ip[valid_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "test_ip = [ip[test_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "del times_inp, values_inp, varis_inp\n",
    "train_op = y[train_ind]\n",
    "valid_op = y[valid_ind]\n",
    "test_op = y[test_ind]\n",
    "del y\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "C8VrWUiiwBBy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
    "def get_res(y_true, y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    minrp = np.minimum(precision, recall).max()\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    return [roc_auc, pr_auc, minrp]\n",
    "\n",
    "########################################################################################################\n",
    "########################################################################################################\n",
    "#class_weights = compute_class_weight(class_weight='balanced', classes=[0,1], y=train_op)\n",
    "def mortality_loss(y_true, y_pred):\n",
    "    sample_weights = (1-y_true)*class_weights[0] + y_true*class_weights[1]\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    return K.mean(sample_weights*bce, axis=-1)\n",
    "########################################################################################################\n",
    "########################################################################################################\n",
    "\n",
    "# var_weights = np.sum(fore_train_op[:, V:], axis=0)\n",
    "# var_weights[var_weights==0] = var_weights.max()\n",
    "# var_weights = var_weights.max()/var_weights\n",
    "# var_weights = var_weights.reshape((1, V))\n",
    "def forecast_loss(y_true, y_pred):\n",
    "    return K.sum(y_true[:,V:]*(y_true[:,:V]-y_pred)**2, axis=-1)\n",
    "\n",
    "def mape_fore(y_true, y_pred):\n",
    "    truth = y_true[:,V:]\n",
    "    pred = y_pred\n",
    "    return mape(truth, pred)\n",
    "\n",
    "                                          \n",
    "def get_min_loss(weight):\n",
    "    def min_loss(y_true, y_pred):\n",
    "        return weight*y_pred\n",
    "    return min_loss\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, validation_data, batch_size):\n",
    "        self.val_x, self.val_y = validation_data\n",
    "        self.batch_size = batch_size\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.val_x, verbose=0, batch_size=self.batch_size)\n",
    "        if type(y_pred)==type([]):\n",
    "            y_pred = y_pred[0]\n",
    "        precision, recall, thresholds = precision_recall_curve(self.val_y, y_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        roc_auc = roc_auc_score(self.val_y, y_pred)\n",
    "        logs['custom_metric'] = pr_auc + roc_auc\n",
    "        print ('val_aucs:', pr_auc, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "isIVij5VwFhk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Activation, Dropout, Softmax, Layer, InputSpec, Input, Dense, Lambda, TimeDistributed, Concatenate, Add\n",
    "from tensorflow.keras import initializers, regularizers, constraints, Model\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow import nn\n",
    "import smart_cond_mod as sc\n",
    "\n",
    "\n",
    "class CVE(Layer):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        super(CVE, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W1 = self.add_weight(name='CVE_W1',\n",
    "                            shape=(1, self.hid_units),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        self.b1 = self.add_weight(name='CVE_b1',\n",
    "                            shape=(self.hid_units,),\n",
    "                            initializer='zeros',\n",
    "                            trainable=True)\n",
    "        self.W2 = self.add_weight(name='CVE_W2',\n",
    "                            shape=(self.hid_units, self.output_dim),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        super(CVE, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = K.expand_dims(x, axis=-1)\n",
    "        x = K.dot(K.tanh(K.bias_add(K.dot(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape + (self.output_dim,)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, hid_dim):\n",
    "        self.hid_dim = hid_dim\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        self.W = self.add_weight(shape=(d, self.hid_dim), name='Att_W',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.hid_dim,), name='Att_b',\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(shape=(self.hid_dim,1), name='Att_u',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = K.dot(K.tanh(K.bias_add(K.dot(x,self.W), self.b)), self.u)\n",
    "        mask = K.expand_dims(mask, axis=-1)\n",
    "        attn_weights = mask*attn_weights + (1-mask)*mask_value\n",
    "        attn_weights = K.softmax(attn_weights, axis=-2)\n",
    "        return attn_weights\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (1,)\n",
    "\n",
    "\n",
    "class Transformer(Layer):\n",
    "\n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0):\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = K.epsilon() * K.epsilon()\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        if self.dk==None:\n",
    "            self.dk = d//self.h\n",
    "        if self.dv==None:\n",
    "            self.dv = d//self.h\n",
    "        if self.dff==None:\n",
    "            self.dff = 2*d\n",
    "        self.Wq = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wq',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wk = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wk',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wv = self.add_weight(shape=(self.N, self.h, d, self.dv), name='Wv',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wo = self.add_weight(shape=(self.N, self.dv*self.h, d), name='Wo',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.W1 = self.add_weight(shape=(self.N, d, self.dff), name='W1',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b1 = self.add_weight(shape=(self.N, self.dff), name='b1',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.W2 = self.add_weight(shape=(self.N, self.dff, d), name='W2',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b2 = self.add_weight(shape=(self.N, d), name='b2',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.gamma = self.add_weight(shape=(2*self.N,), name='gamma',\n",
    "                                 initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(shape=(2*self.N,), name='beta',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Transformer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask, mask_value=-1e-30):\n",
    "        mask = K.expand_dims(mask, axis=-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = K.dot(x, self.Wq[i,j,:,:])\n",
    "                k = K.permute_dimensions(K.dot(x, self.Wk[i,j,:,:]), (0,2,1))\n",
    "                v = K.dot(x, self.Wv[i,j,:,:])\n",
    "                A = K.batch_dot(q,k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask*A + (1-mask)*mask_value\n",
    "                # Mask for attention dropout.\n",
    "                def dropped_A():\n",
    "                    dp_mask = K.cast((K.random_uniform(shape=array_ops.shape(A))>=self.dropout), K.floatx())\n",
    "                    return A*dp_mask + (1-dp_mask)*mask_value\n",
    "                A = sc.smart_cond(K.learning_phase(), dropped_A, lambda: array_ops.identity(A))\n",
    "                A = K.softmax(A, axis=-1)\n",
    "                mha_ops.append(K.batch_dot(A,v))\n",
    "            conc = K.concatenate(mha_ops, axis=-1)\n",
    "            proj = K.dot(conc, self.Wo[i,:,:])\n",
    "            # Dropout.\n",
    "            proj = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(proj, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(proj))\n",
    "            # Add & LN\n",
    "            x = x+proj\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i] + self.beta[2*i]\n",
    "            # FFN\n",
    "            ffn_op = K.bias_add(K.dot(K.relu(K.bias_add(K.dot(x, self.W1[i,:,:]), self.b1[i,:])),\n",
    "                           self.W2[i,:,:]), self.b2[i,:,])\n",
    "            # Dropout.\n",
    "            ffn_op = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(ffn_op, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(ffn_op))\n",
    "            # Add & LN\n",
    "            x = x+ffn_op\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i+1] + self.beta[2*i+1]\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "def build_strats(D, max_len, V, d, N, he, dropout, forecast=False):\n",
    "    demo = Input(shape=(D,))\n",
    "    demo_enc = Dense(2*d, activation='tanh')(demo)\n",
    "    demo_enc = Dense(d, activation='tanh')(demo_enc)\n",
    "    varis = Input(shape=(max_len,))\n",
    "    values = Input(shape=(max_len,))\n",
    "    times = Input(shape=(max_len,))\n",
    "    varis_emb = Embedding(V+1, d)(varis)\n",
    "    cve_units = int(np.sqrt(d))\n",
    "    values_emb = CVE(cve_units, d)(values)\n",
    "    times_emb = CVE(cve_units, d)(times)\n",
    "    comb_emb = Add()([varis_emb, values_emb, times_emb]) # b, L, d\n",
    "#     demo_enc = Lambda(lambda x:K.expand_dims(x, axis=-2))(demo_enc) # b, 1, d\n",
    "#     comb_emb = Concatenate(axis=-2)([demo_enc, comb_emb]) # b, L+1, d\n",
    "    mask = Lambda(lambda x:K.clip(x,0,1))(varis) # b, L\n",
    "#     mask = Lambda(lambda x:K.concatenate((K.ones_like(x)[:,0:1], x), axis=-1))(mask) # b, L+1\n",
    "    cont_emb = Transformer(N, he, dk=None, dv=None, dff=None, dropout=dropout)(comb_emb, mask=mask)\n",
    "    attn_weights = Attention(2*d)(cont_emb, mask=mask)\n",
    "    fused_emb = Lambda(lambda x:K.sum(x[0]*x[1], axis=-2))([cont_emb, attn_weights])\n",
    "    conc = Concatenate(axis=-1)([fused_emb, demo_enc])\n",
    "    fore_op = Dense(V)(conc)\n",
    "    op = Dense(1, activation='sigmoid')(fore_op)\n",
    "    model = Model([demo, times, values, varis], op)\n",
    "    if forecast:\n",
    "        fore_model = Model([demo, times, values, varis], fore_op)\n",
    "        return [model, fore_model]\n",
    "    return model\n",
    "\n",
    "# To tune:\n",
    "# 1. Transformer parameters. (N, h, dropout)\n",
    "# 2. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbYsD4V556kT"
   },
   "source": [
    "## get forecast on test patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sfu0I5Q4ECl-"
   },
   "source": [
    "### Load test data into matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_hours = 24\n",
    "pred_window = 1 # hours that the output vector represents. 1 because i want to learn to predict 1 hour many times, 2 to test standard strats config\n",
    "obs_windows = [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HKK7sW-455hs",
    "outputId": "c637236a-199d-4e54-90e5-2d38698a359d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19788it [00:00, 706074.06it/s]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# take test patients.\n",
    "data = data.merge(oc[['ts_ind', 'SUBJECT_ID']], on='ts_ind', how='left')\n",
    "test_sub = oc.loc[oc.ts_ind.isin(test_ind)].SUBJECT_ID.unique()\n",
    "data = data.loc[data.SUBJECT_ID.isin(test_sub)]\n",
    "oc = oc.loc[oc.SUBJECT_ID.isin(test_sub)]\n",
    "data.drop(columns=['SUBJECT_ID'], inplace=True)\n",
    "# Fix age.\n",
    "data.loc[(data.variable=='Age')&(data.value>200), 'value'] = 91.4\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "N = data.ts_ind.max()+1\n",
    "demo = np.zeros((N, D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds==0)*1 + (stds!=0)*stds\n",
    "demo = (demo-means)/stds\n",
    "\n",
    "# Get variable indices.\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Find max_len.\n",
    "fore_max_len = 880\n",
    "# Get forecast inputs and outputs.\n",
    "fore_times_ip = []\n",
    "fore_values_ip = []\n",
    "fore_varis_ip = []\n",
    "fore_op = []\n",
    "fore_inds = []\n",
    "def f(x):\n",
    "    mask = [0 for i in range(V)]\n",
    "    values = [0 for i in range(V)]\n",
    "    for vv in x:\n",
    "        v = int(vv[0])-1\n",
    "        mask[v] = 1\n",
    "        values[v] = vv[1]\n",
    "    return values+mask\n",
    "def pad(x):\n",
    "    return x+[0]*(V+V*forecast_hours-len(x))\n",
    "\n",
    "for w in tqdm(obs_windows):\n",
    "    pred_data = data.loc[(data.hour>=w)&(data.hour<=w+pred_window)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value':'first'}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value':list}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data['vind_value'].apply(f)\n",
    "    obs_data = data.loc[(data.hour<w)&(data.hour>=w-24)]\n",
    "    obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "    obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "    obs_data = obs_data.groupby('ts_ind').agg({'vind':list, 'hour':list, 'value':list}).reset_index()\n",
    "    obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "    for col in ['vind', 'hour', 'value']:\n",
    "        obs_data[col] = obs_data[col].apply(pad)\n",
    "    fore_op.append(np.array(list(obs_data.vind_value)))\n",
    "    fore_inds.append(np.array(list(obs_data.ts_ind)))\n",
    "    fore_times_ip.append(np.array(list(obs_data.hour)))\n",
    "    fore_values_ip.append(np.array(list(obs_data.value)))\n",
    "    fore_varis_ip.append(np.array(list(obs_data.vind)))\n",
    "\n",
    "del data\n",
    "fore_times_ip = np.concatenate(fore_times_ip, axis=0)\n",
    "fore_values_ip = np.concatenate(fore_values_ip, axis=0)\n",
    "fore_varis_ip = np.concatenate(fore_varis_ip, axis=0)\n",
    "fore_op = np.concatenate(fore_op, axis=0)\n",
    "fore_inds = np.concatenate(fore_inds, axis=0)\n",
    "fore_demo = demo[fore_inds]\n",
    "\n",
    "fore_test_ip = [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]\n",
    "fore_test_op = fore_op\n",
    "\n",
    "# release RAM\n",
    "del fore_times_ip, fore_values_ip, fore_varis_ip, demo, fore_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def repeat_prediction(first_input, y_preds, relative_time, max_seq_len, len_vari=V):\n",
    "    \n",
    "    # build new input with prediction added as \"newest data\"\n",
    "    # take highest \"time\" and add number of forecasting steps -> time of new forecast\n",
    "    preds_time = (first_input[1].max(axis=1, keepdims=True))+relative_time\n",
    "    #print(\"predstime\", preds_time)\n",
    "    # all features are predicted at that same \"new\" time\n",
    "    preds_time = np.repeat(preds_time, len_vari, axis=1)\n",
    "    #print(\"predstime\", preds_time)\n",
    "    # each variable is predicted\n",
    "    preds_varid = np.linspace(1, len_vari, num=V)\n",
    "    #print(\"varid\", preds_varid)\n",
    "    # repeat for each time already predicted\n",
    "    preds_varid = np.tile(preds_varid, (len(preds_time), 1))\n",
    "    #print(\"varid\", preds_varid)\n",
    "    # take the original demographics, as it does not change\n",
    "    preds_demo = first_input[0]\n",
    "    new_data = [preds_demo, preds_time, y_preds, preds_varid]\n",
    "    \n",
    "    \n",
    "    new_fore_ip = [first_input[0]]\n",
    "    # during first run keep first set of features (first_input[i][:,:len_vari])\n",
    "    if relative_time == 0:\n",
    "        for i in range(1,4):\n",
    "            # append the first set of forecast\n",
    "            new_fore_ip.append(np.append(first_input[i][:,:len_vari], new_data[i], axis=1)) #only take existing values w/o padding and combine with prediction\n",
    "        print(len(new_fore_ip[1][0]))\n",
    "    # during consecutive runs keep relative_time+1 set of features (so the first original + forecasts)\n",
    "    else:\n",
    "        for i in range(1,4):\n",
    "            new_fore_ip.append(np.append(first_input[i][:,:len_vari*(relative_time+1)], new_data[i], axis=1)) #only take existing values w/o padding and combine with prediction\n",
    "        print(len(new_fore_ip[1][0]))\n",
    "\n",
    "    # make padding depending on how long the new_fore_ip is. max_seq_len-full set of features during first; max_seq_len-2*(full set of features) during second and so on\n",
    "    f = np.zeros((len(new_fore_ip[1]),(max_seq_len-len(new_fore_ip[1][0])))) # padding\n",
    "    for i in range(1,4):\n",
    "        new_fore_ip[i] = np.append(new_fore_ip[i],f, axis=1) # add padding\n",
    "    \n",
    "    #return to predict again, this time with new data, containing previous predictions and so on\n",
    "    return new_fore_ip, new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for logging purposes\n",
    "start_ip = fore_test_ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovxJ-7-yEG0I"
   },
   "source": [
    "### get test `y_preds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvz1y8m--5HK",
    "outputId": "07ce8037-8437-4878-f361-87e502f5dcee",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 01:42:42.721873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31133 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3a:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 4575)]               0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 4575)]               0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 4575)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 4575, 50)             9200      ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " cve (CVE)                   (None, 4575, 50)             364       ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " cve_1 (CVE)                 (None, 4575, 50)             364       ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 4575, 50)             0         ['embedding[0][0]',           \n",
      "                                                                     'cve[0][0]',                 \n",
      "                                                                     'cve_1[0][0]']               \n",
      "                                                                                                  \n",
      " lambda (Lambda)             (None, 4575)                 0         ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " transformer (Transformer)   (None, 4575, 50)             39508     ['add[0][0]',                 \n",
      "                                                                     'lambda[0][0]']              \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)        [(None, 2)]                  0         []                            \n",
      "                                                                                                  \n",
      " attention (Attention)       (None, 4575, 1)              5200      ['transformer[0][0]',         \n",
      "                                                                     'lambda[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 100)                  300       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)           (None, 50)                   0         ['transformer[0][0]',         \n",
      "                                                                     'attention[0][0]']           \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 50)                   5050      ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 100)                  0         ['lambda_1[0][0]',            \n",
      "                                                                     'dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 183)                  18483     ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 78469 (306.52 KB)\n",
      "Trainable params: 78469 (306.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Restored from ./tfidf_STraTS_20-124_predsize_1V3/ckpt-25\n",
      "301/301 [==============================] - 114s 374ms/step\n",
      "366\n",
      "301/301 [==============================] - 113s 375ms/step\n",
      "549\n",
      "301/301 [==============================] - 113s 375ms/step\n",
      "732\n",
      "301/301 [==============================] - 113s 375ms/step\n",
      "915\n",
      "301/301 [==============================] - 113s 375ms/step\n",
      "1098\n",
      "301/301 [==============================] - 113s 375ms/step\n",
      "1281\n",
      "301/301 [==============================] - 113s 375ms/step\n",
      "1464\n",
      "301/301 [==============================] - 112s 375ms/step\n",
      "1647\n",
      "301/301 [==============================] - 112s 375ms/step\n",
      "1830\n",
      "301/301 [==============================] - 113s 375ms/step\n",
      "2013\n",
      "301/301 [==============================] - 113s 375ms/step\n",
      "2196\n",
      "301/301 [==============================] - 113s 375ms/step\n",
      "2379\n",
      "301/301 [==============================] - 113s 375ms/step\n",
      "2562\n",
      "301/301 [==============================] - 113s 375ms/step\n",
      "2745\n",
      "301/301 [==============================] - 113s 375ms/step\n",
      "2928\n",
      "301/301 [==============================] - 113s 376ms/step\n",
      "3111\n",
      "301/301 [==============================] - 113s 376ms/step\n",
      "3294\n",
      "301/301 [==============================] - 113s 376ms/step\n",
      "3477\n",
      "301/301 [==============================] - 113s 376ms/step\n",
      "3660\n",
      "301/301 [==============================] - 113s 376ms/step\n",
      "3843\n",
      "301/301 [==============================] - 113s 376ms/step\n",
      "4026\n",
      "301/301 [==============================] - 113s 376ms/step\n",
      "4209\n",
      "301/301 [==============================] - 113s 376ms/step\n",
      "4392\n",
      "301/301 [==============================] - 113s 376ms/step\n",
      "4575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9605/9605 [00:04<00:00, 1976.47it/s]\n"
     ]
    }
   ],
   "source": [
    "fore_savepath = './tfidf_STraTS_20-124_predsize_1V3'\n",
    "\n",
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, len(fore_test_op), 5\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model, fore_model =  build_strats(D, V+V*forecast_hours, V, d, N, he, dropout, forecast=True)\n",
    "print(fore_model.summary())\n",
    "# fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "lossfunction = forecast_loss\n",
    "opt = tf.keras.optimizers.Adam(lr)\n",
    "fore_model.compile(loss=lossfunction, optimizer=opt)\n",
    "\n",
    "# initialize checkpoint manager\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=fore_model)\n",
    "manager = tf.train.CheckpointManager(ckpt, f'{fore_savepath}', max_to_keep=3)\n",
    "ckpt.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")\n",
    "\n",
    "# forecasting\n",
    "for i in range(0, forecast_hours):\n",
    "    # predict and save prediction \n",
    "    test_y_preds = fore_model.predict(fore_test_ip)\n",
    "\n",
    "    # during 1st run, initialize cumulated predictions e.g. hour 5\n",
    "    if i == 0:\n",
    "        predictions = test_y_preds\n",
    "        \n",
    "    # during following runs, concatenate predictions\n",
    "    else:\n",
    "        predictions = np.concatenate((predictions,test_y_preds), axis=1)\n",
    "        \n",
    "    # build new input consisting of original input + hourly predictions appended\n",
    "    fore_test_ip, n_data_ = repeat_prediction(fore_test_ip,test_y_preds, i, max_seq_len=V+V*forecast_hours)  \n",
    "    \n",
    "\n",
    "hours = []\n",
    "max_hours = []\n",
    "# get hours\n",
    "for time in start_ip[1]:\n",
    "  hour = max(time)\n",
    "  max_hours.append(hour)\n",
    "\n",
    "  for obs_window in obs_windows:\n",
    "    if hour < obs_window:\n",
    "      hour = obs_window\n",
    "      break\n",
    "\n",
    "  hours.append(hour)\n",
    "\n",
    "# get patient ids\n",
    "test_patient_ids = []\n",
    "test_sepsis_labels = []\n",
    "\n",
    "# sub_ids = oc['SUBJECT_ID'].tolist()\n",
    "# ts_ind = oc['ts_ind'].tolist()\n",
    "# sepsis = oc['in_hospital_sepsis'].tolist()\n",
    "\n",
    "# for ind in val_inds:\n",
    "#    for i in range(len(sub_ids)):\n",
    "#      if ts_ind[i] == ind:\n",
    "#        val_sepsis_labels.append(sepsis[i])\n",
    "#        val_patient_ids.append(sub_ids[i])\n",
    "#        break\n",
    "\n",
    "for ind in tqdm(fore_inds):\n",
    "  test_sepsis_labels.append(np.unique(oc[oc['ts_ind']==ind]['in_hospital_sepsis']).item())\n",
    "  test_patient_ids.append(np.unique(oc[oc['ts_ind']==ind]['SUBJECT_ID']).item())\n",
    "\n",
    "test_data = pd.DataFrame(\n",
    "    {'ts_ind': fore_inds,\n",
    "     'obs_window': hours,\n",
    "     'SUBJECT_ID': test_patient_ids,\n",
    "     'sepsis_label': test_sepsis_labels,\n",
    "     'forecasting_pred': pd.Series(predictions.tolist()),\n",
    "     'forecasting_test_op': pd.Series(fore_op.tolist())\n",
    "    })\n",
    "\n",
    "# dump to pkl\n",
    "pickle.dump([test_data, var_to_ind], open('pred_size1_TF_STraTS_20-124V3_4-28_forecast.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuClass": "premium",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
